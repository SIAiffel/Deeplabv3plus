{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ed12ad0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:59:14.511062Z",
     "start_time": "2021-06-17T06:59:14.508143Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "541aefa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:59:15.009773Z",
     "start_time": "2021-06-17T06:59:14.512690Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchvision.models.utils import load_state_dict_from_url\n",
    "import torch.nn.functional as F\n",
    "\n",
    "__all__ = ['MobileNetV2', 'mobilenet_v2']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'mobilenet_v2': 'https://download.pytorch.org/models/mobilenet_v2-b0353104.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "class ConvBNReLU(nn.Sequential):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, dilation=1, groups=1):\n",
    "        #padding = (kernel_size - 1) // 2\n",
    "        super(ConvBNReLU, self).__init__(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, 0, dilation=dilation, groups=groups, bias=False),\n",
    "            nn.BatchNorm2d(out_planes),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "def fixed_padding(kernel_size, dilation):\n",
    "    kernel_size_effective = kernel_size + (kernel_size - 1) * (dilation - 1)\n",
    "    pad_total = kernel_size_effective - 1\n",
    "    pad_beg = pad_total // 2\n",
    "    pad_end = pad_total - pad_beg\n",
    "    return (pad_beg, pad_end, pad_beg, pad_end) \n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, dilation, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = int(round(inp * expand_ratio))\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        layers = []\n",
    "        if expand_ratio != 1:\n",
    "            # pw\n",
    "            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n",
    "\n",
    "        layers.extend([\n",
    "            # dw\n",
    "            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, dilation=dilation, groups=hidden_dim),\n",
    "            # pw-linear\n",
    "            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(oup),\n",
    "        ])\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "        self.input_padding = fixed_padding( 3, dilation )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_pad = F.pad(x, self.input_padding)\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x_pad)\n",
    "        else:\n",
    "            return self.conv(x_pad)\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=1000, output_stride=8, width_mult=1.0, inverted_residual_setting=None, round_nearest=8):\n",
    "        \"\"\"\n",
    "        MobileNet V2 main class\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): Number of classes\n",
    "            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n",
    "            inverted_residual_setting: Network structure\n",
    "            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n",
    "            Set to 1 to turn off rounding\n",
    "        \"\"\"\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "        self.output_stride = output_stride\n",
    "        current_stride = 1\n",
    "        if inverted_residual_setting is None:\n",
    "            inverted_residual_setting = [\n",
    "                # t, c, n, s\n",
    "                [1, 16, 1, 1],\n",
    "                [6, 24, 2, 2],\n",
    "                [6, 32, 3, 2],\n",
    "                [6, 64, 4, 2],\n",
    "                [6, 96, 3, 1],\n",
    "                [6, 160, 3, 2],\n",
    "                [6, 320, 1, 1],\n",
    "            ]\n",
    "\n",
    "        # only check the first element, assuming user knows t,c,n,s are required\n",
    "        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n",
    "            raise ValueError(\"inverted_residual_setting should be non-empty \"\n",
    "                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n",
    "        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n",
    "        features = [ConvBNReLU(3, input_channel, stride=2)]\n",
    "        current_stride *= 2\n",
    "        dilation=1\n",
    "        previous_dilation = 1\n",
    "\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in inverted_residual_setting:\n",
    "            output_channel = _make_divisible(c * width_mult, round_nearest)\n",
    "            previous_dilation = dilation\n",
    "            if current_stride == output_stride:\n",
    "                stride = 1\n",
    "                dilation *= s\n",
    "            else:\n",
    "                stride = s\n",
    "                current_stride *= s\n",
    "            output_channel = int(c * width_mult)\n",
    "\n",
    "            for i in range(n):\n",
    "                if i==0:\n",
    "                    features.append(block(input_channel, output_channel, stride, previous_dilation, expand_ratio=t))\n",
    "                else:\n",
    "                    features.append(block(input_channel, output_channel, 1, dilation, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*features)\n",
    "\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, num_classes),\n",
    "        )\n",
    "\n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.mean([2, 3])\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def mobilenet_v2(pretrained=False, progress=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a MobileNetV2 architecture from\n",
    "    `\"MobileNetV2: Inverted Residuals and Linear Bottlenecks\" <https://arxiv.org/abs/1801.04381>`_.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    model = MobileNetV2(**kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls['mobilenet_v2'],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d434f9f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:59:15.019084Z",
     "start_time": "2021-06-17T06:59:15.011271Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "class _SimpleSegmentationModel(nn.Module):\n",
    "    def __init__(self, backbone, classifier):\n",
    "        super(_SimpleSegmentationModel, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.classifier = classifier\n",
    "        \n",
    "    def forward(self, x):\n",
    "        input_shape = x.shape[-2:]\n",
    "        print()\n",
    "        features = self.backbone(x)\n",
    "        x = self.classifier(features)\n",
    "        x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IntermediateLayerGetter(nn.ModuleDict):\n",
    "    \"\"\"\n",
    "    Module wrapper that returns intermediate layers from a model\n",
    "\n",
    "    It has a strong assumption that the modules have been registered\n",
    "    into the model in the same order as they are used.\n",
    "    This means that one should **not** reuse the same nn.Module\n",
    "    twice in the forward if you want this to work.\n",
    "\n",
    "    Additionally, it is only able to query submodules that are directly\n",
    "    assigned to the model. So if `model` is passed, `model.feature1` can\n",
    "    be returned, but not `model.feature1.layer2`.\n",
    "\n",
    "    Arguments:\n",
    "        model (nn.Module): model on which we will extract the features\n",
    "        return_layers (Dict[name, new_name]): a dict containing the names\n",
    "            of the modules for which the activations will be returned as\n",
    "            the key of the dict, and the value of the dict is the name\n",
    "            of the returned activation (which the user can specify).\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> m = torchvision.models.resnet18(pretrained=True)\n",
    "        >>> # extract layer1 and layer3, giving as names `feat1` and feat2`\n",
    "        >>> new_m = torchvision.models._utils.IntermediateLayerGetter(m,\n",
    "        >>>     {'layer1': 'feat1', 'layer3': 'feat2'})\n",
    "        >>> out = new_m(torch.rand(1, 3, 224, 224))\n",
    "        >>> print([(k, v.shape) for k, v in out.items()])\n",
    "        >>>     [('feat1', torch.Size([1, 64, 56, 56])),\n",
    "        >>>      ('feat2', torch.Size([1, 256, 14, 14]))]\n",
    "    \"\"\"\n",
    "    def __init__(self, model, return_layers):\n",
    "        if not set(return_layers).issubset([name for name, _ in model.named_children()]):\n",
    "            raise ValueError(\"return_layers are not present in model\")\n",
    "\n",
    "        orig_return_layers = return_layers\n",
    "        return_layers = {k: v for k, v in return_layers.items()}\n",
    "        layers = OrderedDict()\n",
    "        for name, module in model.named_children():\n",
    "            layers[name] = module\n",
    "            if name in return_layers:\n",
    "                del return_layers[name]\n",
    "            if not return_layers:\n",
    "                break\n",
    "\n",
    "        super(IntermediateLayerGetter, self).__init__(layers)\n",
    "        self.return_layers = orig_return_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = OrderedDict()\n",
    "        for name, module in self.named_children():\n",
    "            x = module(x)\n",
    "            if name in self.return_layers:\n",
    "                out_name = self.return_layers[name]\n",
    "                out[out_name] = x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f055350",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:59:15.043566Z",
     "start_time": "2021-06-17T06:59:15.020526Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#from .utils import _SimpleSegmentationModel\n",
    "\n",
    "\n",
    "__all__ = [\"DeepLabV3\"]\n",
    "\n",
    "\n",
    "class DeepLabV3(_SimpleSegmentationModel):\n",
    "    \"\"\"\n",
    "    Implements DeepLabV3 model from\n",
    "    `\"Rethinking Atrous Convolution for Semantic Image Segmentation\"\n",
    "    <https://arxiv.org/abs/1706.05587>`_.\n",
    "\n",
    "    Arguments:\n",
    "        backbone (nn.Module): the network used to compute the features for the model.\n",
    "            The backbone should return an OrderedDict[Tensor], with the key being\n",
    "            \"out\" for the last feature map used, and \"aux\" if an auxiliary classifier\n",
    "            is used.\n",
    "        classifier (nn.Module): module that takes the \"out\" element returned from\n",
    "            the backbone and returns a dense prediction.\n",
    "        aux_classifier (nn.Module, optional): auxiliary classifier used during training\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "class DeepLabHeadV3Plus(nn.Module):\n",
    "    def __init__(self, in_channels, low_level_channels, num_classes, aspp_dilate=[12, 24, 36]):\n",
    "        super(DeepLabHeadV3Plus, self).__init__()\n",
    "        self.project = nn.Sequential( \n",
    "            nn.Conv2d(low_level_channels, 48, 1, bias=False),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.aspp = ASPP(in_channels, aspp_dilate)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(304, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, num_classes, 1)\n",
    "        )\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, feature):\n",
    "        low_level_feature = self.project( feature['low_level'] )\n",
    "        output_feature = self.aspp(feature['out'])\n",
    "        output_feature = F.interpolate(output_feature, size=low_level_feature.shape[2:], mode='bilinear', align_corners=False)\n",
    "        return self.classifier( torch.cat( [ low_level_feature, output_feature ], dim=1 ) )\n",
    "    \n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "class DeepLabHead(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, aspp_dilate=[12, 24, 36]):\n",
    "        super(DeepLabHead, self).__init__()\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            ASPP(in_channels, aspp_dilate),\n",
    "            nn.Conv2d(256, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, num_classes, 1)\n",
    "        )\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, feature):\n",
    "        return self.classifier( feature['out'] )\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "class AtrousSeparableConvolution(nn.Module):\n",
    "    \"\"\" Atrous Separable Convolution\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                            stride=1, padding=0, dilation=1, bias=True):\n",
    "        super(AtrousSeparableConvolution, self).__init__()\n",
    "        self.body = nn.Sequential(\n",
    "            # Separable Conv\n",
    "            nn.Conv2d( in_channels, in_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias, groups=in_channels ),\n",
    "            # PointWise Conv\n",
    "            nn.Conv2d( in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=bias),\n",
    "        )\n",
    "        \n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "class ASPPConv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, dilation):\n",
    "        modules = [\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        super(ASPPConv, self).__init__(*modules)\n",
    "\n",
    "class ASPPPooling(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ASPPPooling, self).__init__(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.shape[-2:]\n",
    "        x = super(ASPPPooling, self).forward(x)\n",
    "        return F.interpolate(x, size=size, mode='bilinear', align_corners=False)\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, atrous_rates):\n",
    "        super(ASPP, self).__init__()\n",
    "        out_channels = 256\n",
    "        modules = []\n",
    "        modules.append(nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)))\n",
    "\n",
    "        rate1, rate2, rate3 = tuple(atrous_rates)\n",
    "        modules.append(ASPPConv(in_channels, out_channels, rate1))\n",
    "        modules.append(ASPPConv(in_channels, out_channels, rate2))\n",
    "        modules.append(ASPPConv(in_channels, out_channels, rate3))\n",
    "        modules.append(ASPPPooling(in_channels, out_channels))\n",
    "\n",
    "        self.convs = nn.ModuleList(modules)\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(5 * out_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = []\n",
    "        for conv in self.convs:\n",
    "            res.append(conv(x))\n",
    "        res = torch.cat(res, dim=1)\n",
    "        return self.project(res)\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_separable_conv(module):\n",
    "    new_module = module\n",
    "    if isinstance(module, nn.Conv2d) and module.kernel_size[0]>1:\n",
    "        new_module = AtrousSeparableConvolution(module.in_channels,\n",
    "                                      module.out_channels, \n",
    "                                      module.kernel_size,\n",
    "                                      module.stride,\n",
    "                                      module.padding,\n",
    "                                      module.dilation,\n",
    "                                      module.bias)\n",
    "    for name, child in module.named_children():\n",
    "        new_module.add_module(name, convert_to_separable_conv(child))\n",
    "    return new_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aa26865",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:59:15.056999Z",
     "start_time": "2021-06-17T06:59:15.045017Z"
    }
   },
   "outputs": [],
   "source": [
    "#    from .utils import IntermediateLayerGetter\n",
    "#    from ._deeplab import DeepLabHead, DeepLabHeadV3Plus, DeepLabV3\n",
    "#    from .backbone import resnet\n",
    "#    from .backbone import mobilenetv2\n",
    "\n",
    "def _segm_resnet(name, backbone_name, num_classes, output_stride, pretrained_backbone):\n",
    "\n",
    "    if output_stride==8:\n",
    "        replace_stride_with_dilation=[False, True, True]\n",
    "        aspp_dilate = [12, 24, 36]\n",
    "    else:\n",
    "        replace_stride_with_dilation=[False, False, True]\n",
    "        aspp_dilate = [6, 12, 18]\n",
    "\n",
    "    backbone = resnet.__dict__[backbone_name](\n",
    "        pretrained=pretrained_backbone,\n",
    "        replace_stride_with_dilation=replace_stride_with_dilation)\n",
    "    \n",
    "    inplanes = 2048\n",
    "    low_level_planes = 256\n",
    "\n",
    "    if name=='deeplabv3plus':\n",
    "        return_layers = {'layer4': 'out', 'layer1': 'low_level'}\n",
    "        classifier = DeepLabHeadV3Plus(inplanes, low_level_planes, num_classes, aspp_dilate)\n",
    "    elif name=='deeplabv3':\n",
    "        return_layers = {'layer4': 'out'}\n",
    "        classifier = DeepLabHead(inplanes , num_classes, aspp_dilate)\n",
    "    backbone = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
    "\n",
    "    model = DeepLabV3(backbone, classifier)\n",
    "    return model\n",
    "\n",
    "def _segm_mobilenet(name, backbone_name, num_classes, output_stride, pretrained_backbone):\n",
    "    if output_stride==8:\n",
    "        aspp_dilate = [12, 24, 36]\n",
    "    else:\n",
    "        aspp_dilate = [6, 12, 18]\n",
    "\n",
    "    backbone = mobilenet_v2(pretrained=pretrained_backbone, output_stride=output_stride)\n",
    "    \n",
    "    # rename layers\n",
    "    backbone.low_level_features = backbone.features[0:4]\n",
    "    backbone.high_level_features = backbone.features[4:-1]\n",
    "    backbone.features = None\n",
    "    backbone.classifier = None\n",
    "\n",
    "    inplanes = 320\n",
    "    low_level_planes = 24\n",
    "    \n",
    "    if name=='deeplabv3plus':\n",
    "        return_layers = {'high_level_features': 'out', 'low_level_features': 'low_level'}\n",
    "        classifier = DeepLabHeadV3Plus(inplanes, low_level_planes, num_classes, aspp_dilate)\n",
    "    elif name=='deeplabv3':\n",
    "        return_layers = {'high_level_features': 'out'}\n",
    "        classifier = DeepLabHead(inplanes , num_classes, aspp_dilate)\n",
    "    backbone = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
    "\n",
    "    model = DeepLabV3(backbone, classifier)\n",
    "    return model\n",
    "\n",
    "def _load_model(arch_type, backbone, num_classes, output_stride, pretrained_backbone):\n",
    "\n",
    "    if backbone=='mobilenetv2':\n",
    "        model = _segm_mobilenet(arch_type, backbone, num_classes, output_stride=output_stride, pretrained_backbone=pretrained_backbone)\n",
    "    elif backbone.startswith('resnet'):\n",
    "        model = _segm_resnet(arch_type, backbone, num_classes, output_stride=output_stride, pretrained_backbone=pretrained_backbone)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return model\n",
    "\n",
    "\n",
    "# Deeplab v3\n",
    "\n",
    "def deeplabv3_resnet50(num_classes=21, output_stride=8, pretrained_backbone=True):\n",
    "    \"\"\"Constructs a DeepLabV3 model with a ResNet-50 backbone.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        output_stride (int): output stride for deeplab.\n",
    "        pretrained_backbone (bool): If True, use the pretrained backbone.\n",
    "    \"\"\"\n",
    "    return _load_model('deeplabv3', 'resnet50', num_classes, output_stride=output_stride, pretrained_backbone=pretrained_backbone)\n",
    "\n",
    "def deeplabv3_resnet101(num_classes=21, output_stride=8, pretrained_backbone=True):\n",
    "    \"\"\"Constructs a DeepLabV3 model with a ResNet-101 backbone.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        output_stride (int): output stride for deeplab.\n",
    "        pretrained_backbone (bool): If True, use the pretrained backbone.\n",
    "    \"\"\"\n",
    "    return _load_model('deeplabv3', 'resnet101', num_classes, output_stride=output_stride, pretrained_backbone=pretrained_backbone)\n",
    "\n",
    "def deeplabv3_mobilenet(num_classes=21, output_stride=8, pretrained_backbone=True, **kwargs):\n",
    "    \"\"\"Constructs a DeepLabV3 model with a MobileNetv2 backbone.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        output_stride (int): output stride for deeplab.\n",
    "        pretrained_backbone (bool): If True, use the pretrained backbone.\n",
    "    \"\"\"\n",
    "    return _load_model('deeplabv3', 'mobilenetv2', num_classes, output_stride=output_stride, pretrained_backbone=pretrained_backbone)\n",
    "\n",
    "\n",
    "# Deeplab v3+\n",
    "\n",
    "def deeplabv3plus_resnet50(num_classes=21, output_stride=8, pretrained_backbone=True):\n",
    "    \"\"\"Constructs a DeepLabV3 model with a ResNet-50 backbone.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        output_stride (int): output stride for deeplab.\n",
    "        pretrained_backbone (bool): If True, use the pretrained backbone.\n",
    "    \"\"\"\n",
    "    return _load_model('deeplabv3plus', 'resnet50', num_classes, output_stride=output_stride, pretrained_backbone=pretrained_backbone)\n",
    "\n",
    "\n",
    "def deeplabv3plus_resnet101(num_classes=21, output_stride=8, pretrained_backbone=True):\n",
    "    \"\"\"Constructs a DeepLabV3+ model with a ResNet-101 backbone.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        output_stride (int): output stride for deeplab.\n",
    "        pretrained_backbone (bool): If True, use the pretrained backbone.\n",
    "    \"\"\"\n",
    "    return _load_model('deeplabv3plus', 'resnet101', num_classes, output_stride=output_stride, pretrained_backbone=pretrained_backbone)\n",
    "\n",
    "\n",
    "def deeplabv3plus_mobilenet(num_classes=21, output_stride=8, pretrained_backbone=True):\n",
    "    \"\"\"Constructs a DeepLabV3+ model with a MobileNetv2 backbone.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        output_stride (int): output stride for deeplab.\n",
    "        pretrained_backbone (bool): If True, use the pretrained backbone.\n",
    "    \"\"\"\n",
    "    return _load_model('deeplabv3plus', 'mobilenetv2', num_classes, output_stride=output_stride, pretrained_backbone=pretrained_backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "988d68e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:59:15.059947Z",
     "start_time": "2021-06-17T06:59:15.058230Z"
    }
   },
   "outputs": [],
   "source": [
    "#import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcc8bb89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:59:15.063935Z",
     "start_time": "2021-06-17T06:59:15.061884Z"
    }
   },
   "outputs": [],
   "source": [
    "#CKPT_PATH =\"/home/aiffel-dj54/aiffel/siaiffel/DeepLabV3Plus-Pytorch-master/checkpoints/best_deeplabv3plus_mobilenet_satellites_multi_os16.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5188f6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:59:15.068362Z",
     "start_time": "2021-06-17T06:59:15.065769Z"
    }
   },
   "outputs": [],
   "source": [
    "#import network\n",
    "model_map = {\n",
    "        'deeplabv3_resnet50': deeplabv3_resnet50,\n",
    "        'deeplabv3plus_resnet50': deeplabv3plus_resnet50,\n",
    "        'deeplabv3_resnet101': deeplabv3_resnet101,\n",
    "        'deeplabv3plus_resnet101': deeplabv3plus_resnet101,\n",
    "        'deeplabv3_mobilenet': deeplabv3_mobilenet,\n",
    "        'deeplabv3plus_mobilenet': deeplabv3plus_mobilenet\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "674639e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:59:15.233396Z",
     "start_time": "2021-06-17T06:59:15.070002Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepLabHeadV3Plus(\n",
       "  (project): Sequential(\n",
       "    (0): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (aspp): ASPP(\n",
       "    (convs): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): ASPPConv(\n",
       "        (0): AtrousSeparableConvolution(\n",
       "          (body): Sequential(\n",
       "            (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), groups=320, bias=False)\n",
       "            (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): ASPPConv(\n",
       "        (0): AtrousSeparableConvolution(\n",
       "          (body): Sequential(\n",
       "            (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), groups=320, bias=False)\n",
       "            (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): ASPPConv(\n",
       "        (0): AtrousSeparableConvolution(\n",
       "          (body): Sequential(\n",
       "            (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18), groups=320, bias=False)\n",
       "            (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): ASPPPooling(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (project): Sequential(\n",
       "      (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): AtrousSeparableConvolution(\n",
       "      (body): Sequential(\n",
       "        (0): Conv2d(304, 304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=304, bias=False)\n",
       "        (1): Conv2d(304, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = model_map['deeplabv3plus_mobilenet'](num_classes=3, output_stride=16)\n",
    "convert_to_separable_conv(model_1.classifier)\n",
    "#model.load_state_dict(checkpoint[\"model_state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b88536d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:59:15.243908Z",
     "start_time": "2021-06-17T06:59:15.234722Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepLabV3(\n",
       "  (backbone): IntermediateLayerGetter(\n",
       "    (low_level_features): Sequential(\n",
       "      (0): ConvBNReLU(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace=True)\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), groups=96, bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (high_level_features): Sequential(\n",
       "      (4): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (7): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (8): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (9): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (10): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (11): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (12): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (13): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (14): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (15): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (16): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (17): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): DeepLabHeadV3Plus(\n",
       "    (project): Sequential(\n",
       "      (0): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (aspp): ASPP(\n",
       "      (convs): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): ASPPConv(\n",
       "          (0): AtrousSeparableConvolution(\n",
       "            (body): Sequential(\n",
       "              (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), groups=320, bias=False)\n",
       "              (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): ASPPConv(\n",
       "          (0): AtrousSeparableConvolution(\n",
       "            (body): Sequential(\n",
       "              (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), groups=320, bias=False)\n",
       "              (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): ASPPConv(\n",
       "          (0): AtrousSeparableConvolution(\n",
       "            (body): Sequential(\n",
       "              (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18), groups=320, bias=False)\n",
       "              (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): ASPPPooling(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (project): Sequential(\n",
       "        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (classifier): Sequential(\n",
       "      (0): AtrousSeparableConvolution(\n",
       "        (body): Sequential(\n",
       "          (0): Conv2d(304, 304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=304, bias=False)\n",
       "          (1): Conv2d(304, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6b688cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:59:15.298227Z",
     "start_time": "2021-06-17T06:59:15.245421Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "log_dir = \"./runs/new_one\"\n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8047aa78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:59:15.346875Z",
     "start_time": "2021-06-17T06:59:15.299575Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "import random \n",
    "import numbers\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "#\n",
    "#  Extended Transforms for Semantic Segmentation\n",
    "#\n",
    "class ExtRandomHorizontalFlip(object):\n",
    "    \"\"\"Horizontally flip the given PIL Image randomly with a given probability.\n",
    "\n",
    "    Args:\n",
    "        p (float): probability of the image being flipped. Default value is 0.5\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, lbl):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be flipped.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image: Randomly flipped image.\n",
    "        \"\"\"\n",
    "        if random.random() < self.p:\n",
    "            return F.hflip(img), F.hflip(lbl)\n",
    "        return img, lbl\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(p={})'.format(self.p)\n",
    "\n",
    "\n",
    "\n",
    "class ExtCompose(object):\n",
    "    \"\"\"Composes several transforms together.\n",
    "    Args:\n",
    "        transforms (list of ``Transform`` objects): list of transforms to compose.\n",
    "    Example:\n",
    "        >>> transforms.Compose([\n",
    "        >>>     transforms.CenterCrop(10),\n",
    "        >>>     transforms.ToTensor(),\n",
    "        >>> ])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, lbl):\n",
    "        for t in self.transforms:\n",
    "            img, lbl = t(img, lbl)\n",
    "        return img, lbl\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '('\n",
    "        for t in self.transforms:\n",
    "            format_string += '\\n'\n",
    "            format_string += '    {0}'.format(t)\n",
    "        format_string += '\\n)'\n",
    "        return format_string\n",
    "\n",
    "\n",
    "class ExtCenterCrop(object):\n",
    "    \"\"\"Crops the given PIL Image at the center.\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size of the crop. If size is an\n",
    "            int instead of sequence like (h, w), a square crop (size, size) is\n",
    "            made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "\n",
    "    def __call__(self, img, lbl):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be cropped.\n",
    "        Returns:\n",
    "            PIL Image: Cropped image.\n",
    "        \"\"\"\n",
    "        return F.center_crop(img, self.size), F.center_crop(lbl, self.size)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(size={0})'.format(self.size)\n",
    "\n",
    "\n",
    "class ExtRandomScale(object):\n",
    "    def __init__(self, scale_range, interpolation=Image.BILINEAR):\n",
    "        self.scale_range = scale_range\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, img, lbl):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be scaled.\n",
    "            lbl (PIL Image): Label to be scaled.\n",
    "        Returns:\n",
    "            PIL Image: Rescaled image.\n",
    "            PIL Image: Rescaled label.\n",
    "        \"\"\"\n",
    "        assert img.size == lbl.size\n",
    "        scale = random.uniform(self.scale_range[0], self.scale_range[1])\n",
    "        target_size = ( int(img.size[1]*scale), int(img.size[0]*scale) )\n",
    "        return F.resize(img, target_size, self.interpolation), F.resize(lbl, target_size, Image.NEAREST)\n",
    "\n",
    "    def __repr__(self):\n",
    "        interpolate_str = _pil_interpolation_to_str[self.interpolation]\n",
    "        return self.__class__.__name__ + '(size={0}, interpolation={1})'.format(self.size, interpolate_str)\n",
    "\n",
    "class ExtScale(object):\n",
    "    \"\"\"Resize the input PIL Image to the given scale.\n",
    "    Args:\n",
    "        Scale (sequence or int): scale factors\n",
    "        interpolation (int, optional): Desired interpolation. Default is\n",
    "            ``PIL.Image.BILINEAR``\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale, interpolation=Image.BILINEAR):\n",
    "        self.scale = scale\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, img, lbl):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be scaled.\n",
    "            lbl (PIL Image): Label to be scaled.\n",
    "        Returns:\n",
    "            PIL Image: Rescaled image.\n",
    "            PIL Image: Rescaled label.\n",
    "        \"\"\"\n",
    "        assert img.size == lbl.size\n",
    "        target_size = ( int(img.size[1]*self.scale), int(img.size[0]*self.scale) ) # (H, W)\n",
    "        return F.resize(img, target_size, self.interpolation), F.resize(lbl, target_size, Image.NEAREST)\n",
    "\n",
    "    def __repr__(self):\n",
    "        interpolate_str = _pil_interpolation_to_str[self.interpolation]\n",
    "        return self.__class__.__name__ + '(size={0}, interpolation={1})'.format(self.size, interpolate_str)\n",
    "\n",
    "\n",
    "class ExtRandomRotation(object):\n",
    "    \"\"\"Rotate the image by angle.\n",
    "    Args:\n",
    "        degrees (sequence or float or int): Range of degrees to select from.\n",
    "            If degrees is a number instead of sequence like (min, max), the range of degrees\n",
    "            will be (-degrees, +degrees).\n",
    "        resample ({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, optional):\n",
    "            An optional resampling filter.\n",
    "            See http://pillow.readthedocs.io/en/3.4.x/handbook/concepts.html#filters\n",
    "            If omitted, or if the image has mode \"1\" or \"P\", it is set to PIL.Image.NEAREST.\n",
    "        expand (bool, optional): Optional expansion flag.\n",
    "            If true, expands the output to make it large enough to hold the entire rotated image.\n",
    "            If false or omitted, make the output image the same size as the input image.\n",
    "            Note that the expand flag assumes rotation around the center and no translation.\n",
    "        center (2-tuple, optional): Optional center of rotation.\n",
    "            Origin is the upper left corner.\n",
    "            Default is the center of the image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, degrees, resample=False, expand=False, center=None):\n",
    "        if isinstance(degrees, numbers.Number):\n",
    "            if degrees < 0:\n",
    "                raise ValueError(\"If degrees is a single number, it must be positive.\")\n",
    "            self.degrees = (-degrees, degrees)\n",
    "        else:\n",
    "            if len(degrees) != 2:\n",
    "                raise ValueError(\"If degrees is a sequence, it must be of len 2.\")\n",
    "            self.degrees = degrees\n",
    "\n",
    "        self.resample = resample\n",
    "        self.expand = expand\n",
    "        self.center = center\n",
    "\n",
    "    @staticmethod\n",
    "    def get_params(degrees):\n",
    "        \"\"\"Get parameters for ``rotate`` for a random rotation.\n",
    "        Returns:\n",
    "            sequence: params to be passed to ``rotate`` for random rotation.\n",
    "        \"\"\"\n",
    "        angle = random.uniform(degrees[0], degrees[1])\n",
    "\n",
    "        return angle\n",
    "\n",
    "    def __call__(self, img, lbl):\n",
    "        \"\"\"\n",
    "            img (PIL Image): Image to be rotated.\n",
    "            lbl (PIL Image): Label to be rotated.\n",
    "        Returns:\n",
    "            PIL Image: Rotated image.\n",
    "            PIL Image: Rotated label.\n",
    "        \"\"\"\n",
    "\n",
    "        angle = self.get_params(self.degrees)\n",
    "\n",
    "        return F.rotate(img, angle, self.resample, self.expand, self.center), F.rotate(lbl, angle, self.resample, self.expand, self.center)\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '(degrees={0}'.format(self.degrees)\n",
    "        format_string += ', resample={0}'.format(self.resample)\n",
    "        format_string += ', expand={0}'.format(self.expand)\n",
    "        if self.center is not None:\n",
    "            format_string += ', center={0}'.format(self.center)\n",
    "        format_string += ')'\n",
    "        return format_string\n",
    "\n",
    "class ExtRandomHorizontalFlip(object):\n",
    "    \"\"\"Horizontally flip the given PIL Image randomly with a given probability.\n",
    "    Args:\n",
    "        p (float): probability of the image being flipped. Default value is 0.5\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, lbl):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be flipped.\n",
    "        Returns:\n",
    "            PIL Image: Randomly flipped image.\n",
    "        \"\"\"\n",
    "        if random.random() < self.p:\n",
    "            return F.hflip(img), F.hflip(lbl)\n",
    "        return img, lbl\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(p={})'.format(self.p)\n",
    "\n",
    "\n",
    "class ExtRandomVerticalFlip(object):\n",
    "    \"\"\"Vertically flip the given PIL Image randomly with a given probability.\n",
    "    Args:\n",
    "        p (float): probability of the image being flipped. Default value is 0.5\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, lbl):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be flipped.\n",
    "            lbl (PIL Image): Label to be flipped.\n",
    "        Returns:\n",
    "            PIL Image: Randomly flipped image.\n",
    "            PIL Image: Randomly flipped label.\n",
    "        \"\"\"\n",
    "        if random.random() < self.p:\n",
    "            return F.vflip(img), F.vflip(lbl)\n",
    "        return img, lbl\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(p={})'.format(self.p)\n",
    "\n",
    "class ExtPad(object):\n",
    "    def __init__(self, diviser=32):\n",
    "        self.diviser = diviser\n",
    "    \n",
    "    def __call__(self, img, lbl):\n",
    "        h, w = img.size\n",
    "        ph = (h//32+1)*32 - h if h%32!=0 else 0\n",
    "        pw = (w//32+1)*32 - w if w%32!=0 else 0\n",
    "        im = F.pad(img, ( pw//2, pw-pw//2, ph//2, ph-ph//2) )\n",
    "        lbl = F.pad(lbl, ( pw//2, pw-pw//2, ph//2, ph-ph//2))\n",
    "        return im, lbl\n",
    "\n",
    "class ExtToTensor(object):\n",
    "    \"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\n",
    "    Converts a PIL Image or numpy.ndarray (H x W x C) in the range\n",
    "    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
    "    \"\"\"\n",
    "    def __init__(self, normalize=True, target_type='uint8'):\n",
    "        self.normalize = normalize\n",
    "        self.target_type = target_type\n",
    "    def __call__(self, pic, lbl):\n",
    "        \"\"\"\n",
    "        Note that labels will not be normalized to [0, 1].\n",
    "        Args:\n",
    "            pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\n",
    "            lbl (PIL Image or numpy.ndarray): Label to be converted to tensor. \n",
    "        Returns:\n",
    "            Tensor: Converted image and label\n",
    "        \"\"\"\n",
    "        if self.normalize:\n",
    "            return F.to_tensor(pic), torch.from_numpy( np.array( lbl, dtype=self.target_type) )\n",
    "        else:\n",
    "            return torch.from_numpy( np.array( pic, dtype=np.float32).transpose(2, 0, 1) ), torch.from_numpy( np.array( lbl, dtype=self.target_type) )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'\n",
    "\n",
    "class ExtNormalize(object):\n",
    "    \"\"\"Normalize a tensor image with mean and standard deviation.\n",
    "    Given mean: ``(M1,...,Mn)`` and std: ``(S1,..,Sn)`` for ``n`` channels, this transform\n",
    "    will normalize each channel of the input ``torch.*Tensor`` i.e.\n",
    "    ``input[channel] = (input[channel] - mean[channel]) / std[channel]``\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for each channel.\n",
    "        std (sequence): Sequence of standard deviations for each channel.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor, lbl):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "            tensor (Tensor): Tensor of label. A dummy input for ExtCompose\n",
    "        Returns:\n",
    "            Tensor: Normalized Tensor image.\n",
    "            Tensor: Unchanged Tensor label\n",
    "        \"\"\"\n",
    "        return F.normalize(tensor, self.mean, self.std), lbl\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "\n",
    "\n",
    "class ExtRandomCrop(object):\n",
    "    \"\"\"Crop the given PIL Image at a random location.\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size of the crop. If size is an\n",
    "            int instead of sequence like (h, w), a square crop (size, size) is\n",
    "            made.\n",
    "        padding (int or sequence, optional): Optional padding on each border\n",
    "            of the image. Default is 0, i.e no padding. If a sequence of length\n",
    "            4 is provided, it is used to pad left, top, right, bottom borders\n",
    "            respectively.\n",
    "        pad_if_needed (boolean): It will pad the image if smaller than the\n",
    "            desired size to avoid raising an exception.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, padding=0, pad_if_needed=False):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "        self.padding = padding\n",
    "        self.pad_if_needed = pad_if_needed\n",
    "\n",
    "    @staticmethod\n",
    "    def get_params(img, output_size):\n",
    "        \"\"\"Get parameters for ``crop`` for a random crop.\n",
    "        Args:\n",
    "            img (PIL Image): Image to be cropped.\n",
    "            output_size (tuple): Expected output size of the crop.\n",
    "        Returns:\n",
    "            tuple: params (i, j, h, w) to be passed to ``crop`` for random crop.\n",
    "        \"\"\"\n",
    "        w, h = img.size\n",
    "        th, tw = output_size\n",
    "        if w == tw and h == th:\n",
    "            return 0, 0, h, w\n",
    "\n",
    "        i = random.randint(0, h - th)\n",
    "        j = random.randint(0, w - tw)\n",
    "        return i, j, th, tw\n",
    "\n",
    "    def __call__(self, img, lbl):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be cropped.\n",
    "            lbl (PIL Image): Label to be cropped.\n",
    "        Returns:\n",
    "            PIL Image: Cropped image.\n",
    "            PIL Image: Cropped label.\n",
    "        \"\"\"\n",
    "        assert img.size == lbl.size, 'size of img and lbl should be the same. %s, %s'%(img.size, lbl.size)\n",
    "        if self.padding > 0:\n",
    "            img = F.pad(img, self.padding)\n",
    "            lbl = F.pad(lbl, self.padding)\n",
    "\n",
    "        # pad the width if needed\n",
    "        if self.pad_if_needed and img.size[0] < self.size[1]:\n",
    "            img = F.pad(img, padding=int((1 + self.size[1] - img.size[0]) / 2))\n",
    "            lbl = F.pad(lbl, padding=int((1 + self.size[1] - lbl.size[0]) / 2))\n",
    "\n",
    "        # pad the height if needed\n",
    "        if self.pad_if_needed and img.size[1] < self.size[0]:\n",
    "            img = F.pad(img, padding=int((1 + self.size[0] - img.size[1]) / 2))\n",
    "            lbl = F.pad(lbl, padding=int((1 + self.size[0] - lbl.size[1]) / 2))\n",
    "\n",
    "        i, j, h, w = self.get_params(img, self.size)\n",
    "\n",
    "        return F.crop(img, i, j, h, w), F.crop(lbl, i, j, h, w)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(size={0}, padding={1})'.format(self.size, self.padding)\n",
    "\n",
    "\n",
    "class ExtResize(object):\n",
    "    \"\"\"Resize the input PIL Image to the given size.\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size. If size is a sequence like\n",
    "            (h, w), output size will be matched to this. If size is an int,\n",
    "            smaller edge of the image will be matched to this number.\n",
    "            i.e, if height > width, then image will be rescaled to\n",
    "            (size * height / width, size)\n",
    "        interpolation (int, optional): Desired interpolation. Default is\n",
    "            ``PIL.Image.BILINEAR``\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
    "        assert isinstance(size, int) or (isinstance(size, collections.Iterable) and len(size) == 2)\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, img, lbl):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be scaled.\n",
    "        Returns:\n",
    "            PIL Image: Rescaled image.\n",
    "        \"\"\"\n",
    "        return F.resize(img, self.size, self.interpolation), F.resize(lbl, self.size, Image.NEAREST)\n",
    "\n",
    "    def __repr__(self):\n",
    "        interpolate_str = _pil_interpolation_to_str[self.interpolation]\n",
    "        return self.__class__.__name__ + '(size={0}, interpolation={1})'.format(self.size, interpolate_str) \n",
    "    \n",
    "class ExtColorJitter(object):\n",
    "    \"\"\"Randomly change the brightness, contrast and saturation of an image.\n",
    "\n",
    "    Args:\n",
    "        brightness (float or tuple of float (min, max)): How much to jitter brightness.\n",
    "            brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness]\n",
    "            or the given [min, max]. Should be non negative numbers.\n",
    "        contrast (float or tuple of float (min, max)): How much to jitter contrast.\n",
    "            contrast_factor is chosen uniformly from [max(0, 1 - contrast), 1 + contrast]\n",
    "            or the given [min, max]. Should be non negative numbers.\n",
    "        saturation (float or tuple of float (min, max)): How much to jitter saturation.\n",
    "            saturation_factor is chosen uniformly from [max(0, 1 - saturation), 1 + saturation]\n",
    "            or the given [min, max]. Should be non negative numbers.\n",
    "        hue (float or tuple of float (min, max)): How much to jitter hue.\n",
    "            hue_factor is chosen uniformly from [-hue, hue] or the given [min, max].\n",
    "            Should have 0<= hue <= 0.5 or -0.5 <= min <= max <= 0.5.\n",
    "    \"\"\"\n",
    "    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n",
    "        self.brightness = self._check_input(brightness, 'brightness')\n",
    "        self.contrast = self._check_input(contrast, 'contrast')\n",
    "        self.saturation = self._check_input(saturation, 'saturation')\n",
    "        self.hue = self._check_input(hue, 'hue', center=0, bound=(-0.5, 0.5),\n",
    "                                     clip_first_on_zero=False)\n",
    "\n",
    "    def _check_input(self, value, name, center=1, bound=(0, float('inf')), clip_first_on_zero=True):\n",
    "        if isinstance(value, numbers.Number):\n",
    "            if value < 0:\n",
    "                raise ValueError(\"If {} is a single number, it must be non negative.\".format(name))\n",
    "            value = [center - value, center + value]\n",
    "            if clip_first_on_zero:\n",
    "                value[0] = max(value[0], 0)\n",
    "        elif isinstance(value, (tuple, list)) and len(value) == 2:\n",
    "            if not bound[0] <= value[0] <= value[1] <= bound[1]:\n",
    "                raise ValueError(\"{} values should be between {}\".format(name, bound))\n",
    "        else:\n",
    "            raise TypeError(\"{} should be a single number or a list/tuple with lenght 2.\".format(name))\n",
    "\n",
    "        # if value is 0 or (1., 1.) for brightness/contrast/saturation\n",
    "        # or (0., 0.) for hue, do nothing\n",
    "        if value[0] == value[1] == center:\n",
    "            value = None\n",
    "        return value\n",
    "\n",
    "    @staticmethod\n",
    "    def get_params(brightness, contrast, saturation, hue):\n",
    "        \"\"\"Get a randomized transform to be applied on image.\n",
    "\n",
    "        Arguments are same as that of __init__.\n",
    "\n",
    "        Returns:\n",
    "            Transform which randomly adjusts brightness, contrast and\n",
    "            saturation in a random order.\n",
    "        \"\"\"\n",
    "        transforms = []\n",
    "\n",
    "        if brightness is not None:\n",
    "            brightness_factor = random.uniform(brightness[0], brightness[1])\n",
    "            transforms.append(Lambda(lambda img: F.adjust_brightness(img, brightness_factor)))\n",
    "\n",
    "        if contrast is not None:\n",
    "            contrast_factor = random.uniform(contrast[0], contrast[1])\n",
    "            transforms.append(Lambda(lambda img: F.adjust_contrast(img, contrast_factor)))\n",
    "\n",
    "        if saturation is not None:\n",
    "            saturation_factor = random.uniform(saturation[0], saturation[1])\n",
    "            transforms.append(Lambda(lambda img: F.adjust_saturation(img, saturation_factor)))\n",
    "\n",
    "        if hue is not None:\n",
    "            hue_factor = random.uniform(hue[0], hue[1])\n",
    "            transforms.append(Lambda(lambda img: F.adjust_hue(img, hue_factor)))\n",
    "\n",
    "        random.shuffle(transforms)\n",
    "        transform = Compose(transforms)\n",
    "\n",
    "        return transform\n",
    "\n",
    "    def __call__(self, img, lbl):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Input image.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image: Color jittered image.\n",
    "        \"\"\"\n",
    "        transform = self.get_params(self.brightness, self.contrast,\n",
    "                                    self.saturation, self.hue)\n",
    "        return transform(img), lbl\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '('\n",
    "        format_string += 'brightness={0}'.format(self.brightness)\n",
    "        format_string += ', contrast={0}'.format(self.contrast)\n",
    "        format_string += ', saturation={0}'.format(self.saturation)\n",
    "        format_string += ', hue={0})'.format(self.hue)\n",
    "        return format_string\n",
    "\n",
    "class Lambda(object):\n",
    "    \"\"\"Apply a user-defined lambda as a transform.\n",
    "\n",
    "    Args:\n",
    "        lambd (function): Lambda/function to be used for transform.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lambd):\n",
    "        assert callable(lambd), repr(type(lambd).__name__) + \" object is not callable\"\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.lambd(img)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    \"\"\"Composes several transforms together.\n",
    "\n",
    "    Args:\n",
    "        transforms (list of ``Transform`` objects): list of transforms to compose.\n",
    "\n",
    "    Example:\n",
    "        >>> transforms.Compose([\n",
    "        >>>     transforms.CenterCrop(10),\n",
    "        >>>     transforms.ToTensor(),\n",
    "        >>> ])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img):\n",
    "        for t in self.transforms:\n",
    "            img = t(img)\n",
    "        return img\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '('\n",
    "        for t in self.transforms:\n",
    "            format_string += '\\n'\n",
    "            format_string += '    {0}'.format(t)\n",
    "        format_string += '\\n)'\n",
    "        return format_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8adb27a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:59:15.555741Z",
     "start_time": "2021-06-17T06:59:15.348264Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import namedtuple\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Satellites(data.Dataset):\n",
    "    \"\"\"Cityscapes <http://www.cityscapes-dataset.com/> Dataset.\n",
    "    \n",
    "    **Parameters:**\n",
    "        - **root** (string): Root directory of dataset where directory 'leftImg8bit' and 'gtFine' or 'gtCoarse' are located.\n",
    "        - **split** (string, optional): The image split to use, 'train', 'test' or 'val' if mode=\"gtFine\" otherwise 'train', 'train_extra' or 'val'\n",
    "        - **mode** (string, optional): The quality mode to use, 'gtFine' or 'gtCoarse' or 'color'. Can also be a list to output a tuple with all specified target types.\n",
    "        - **transform** (callable, optional): A function/transform that takes in a PIL image and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        - **target_transform** (callable, optional): A function/transform that takes in the target and transforms it.\n",
    "    \"\"\"\n",
    "\n",
    "    # Based on sia\n",
    "    SatellitesClass = namedtuple('SatellitesClass', ['name', 'id', 'train_id', 'category', 'category_id',\n",
    "                                                     'has_instances', 'ignore_in_eval', 'color'])\n",
    "    classes = [\n",
    "        SatellitesClass('unlabeled',            0, 254, 'void', 0, False, False, (0, 0, 0)),\n",
    "        SatellitesClass('building',             1, 1, 'flat', 1, True, False, (128, 128, 0)),\n",
    "        SatellitesClass('road',                 2, 2, 'flat', 1, True, False, (0, 64, 128)),\n",
    "\n",
    "    ]\n",
    "\n",
    "    train_id_to_color = [c.color for c in classes if (c.train_id != -1 and c.train_id != 255)]\n",
    "    train_id_to_color.append([0, 0, 0])\n",
    "    train_id_to_color = np.array(train_id_to_color)\n",
    "    id_to_train_id = np.array([c.train_id for c in classes])\n",
    "    \n",
    "    #train_id_to_color = [(0, 0, 0), (128, 64, 128), (70, 70, 70), (153, 153, 153), (107, 142, 35),\n",
    "    #                      (70, 130, 180), (220, 20, 60), (0, 0, 142)]\n",
    "    #train_id_to_color = np.array(train_id_to_color)\n",
    "    #id_to_train_id = np.array([c.category_id for c in classes], dtype='uint8') - 1\n",
    "\n",
    "    def __init__(self, root, split='train', mode='fine', target_type='sia', transform=None):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.mode = 'gtFine'\n",
    "        self.target_type = target_type\n",
    "        self.images_dir = os.path.join(self.root, 'leftImg8bit', split)\n",
    "\n",
    "        self.targets_dir = os.path.join(self.root, self.mode, split)\n",
    "        self.transform = transform\n",
    "\n",
    "        self.split = split\n",
    "        self.images = []\n",
    "        self.targets = []\n",
    "\n",
    "        if split not in ['train', 'test', 'val']:\n",
    "            raise ValueError('Invalid split for mode! Please use split=\"train\", split=\"test\"'\n",
    "                             ' or split=\"val\"')\n",
    "\n",
    "        if not os.path.isdir(self.images_dir) or not os.path.isdir(self.targets_dir):\n",
    "            raise RuntimeError('Dataset not found or incomplete. Please make sure all required folders for the'\n",
    "                               ' specified \"split\" and \"mode\" are inside the \"root\" directory')\n",
    "        \n",
    "        for city in os.listdir(self.images_dir):\n",
    "            img_dir = os.path.join(self.images_dir, city)\n",
    "            target_dir = os.path.join(self.targets_dir, city)\n",
    "\n",
    "            for file_name in os.listdir(img_dir):\n",
    "                self.images.append(os.path.join(img_dir, file_name))\n",
    "                target_name = '{}{}'.format(file_name.split('.')[0],\n",
    "                                             self._get_target_suffix(self.mode, self.target_type))\n",
    "                self.targets.append(os.path.join(target_dir, target_name))\n",
    "\n",
    "    @classmethod\n",
    "    def encode_target(cls, target):\n",
    "        return cls.id_to_train_id[np.array(target)]\n",
    "\n",
    "    @classmethod\n",
    "    def decode_target(cls, target):\n",
    "        target[target == 255] = 1\n",
    "        #target = target.astype('uint8') + 1\n",
    "        return cls.train_id_to_color[target]\n",
    "    \n",
    "    def make_encode_target(self, target):\n",
    "        #building\n",
    "        target[target == 128] = 1\n",
    "        \n",
    "        #road\n",
    "        #target = cv2.cvtColor(target, cv2.COLOR_BGR2GRAY)\n",
    "        #target[target == 255] = 2\n",
    "        \n",
    "        return target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is a tuple of all target types if target_type is a list with more\n",
    "            than one item. Otherwise target is a json object if target_type=\"polygon\", else the image segmentation.\n",
    "        \"\"\"\n",
    "        image = Image.open(self.images[index]).convert('RGB')\n",
    "        target = Image.open(self.targets[index])\n",
    "        \n",
    "        if self.transform:\n",
    "            image, target = self.transform(image, target)\n",
    "        target = np.array(target)\n",
    "        target = self.make_encode_target(target)\n",
    "        target = np.array(target, dtype='int')\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "    def __getimg__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is a tuple of all target types if target_type is a list with more\n",
    "            than one item. Otherwise target is a json object if target_type=\"polygon\", else the image segmentation.\n",
    "        \"\"\"\n",
    "        image = Image.open(self.images[index]).convert('RGB')\n",
    "        print(self.images[index])\n",
    "        #target = Image.open(self.targets[index])\n",
    "        #if self.transform:\n",
    "        #    image, target = self.transform(image, target)\n",
    "        #target = self.encode_target(target)\n",
    "        return image #, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def _load_json(self, path):\n",
    "        with open(path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "\n",
    "    def _get_target_suffix(self, mode, target_type):\n",
    "        if target_type == 'instance':\n",
    "            return '{}_instanceIds.png'.format(mode)\n",
    "        elif target_type == 'semantic':\n",
    "            return '{}_labelIds.png'.format(mode)\n",
    "        elif target_type == 'color':\n",
    "            return '{}_color.png'.format(mode)\n",
    "        elif target_type == 'polygon':\n",
    "            return '{}_polygons.json'.format(mode)\n",
    "        elif target_type == 'depth':\n",
    "            return '{}_disparity.png'.format(mode)\n",
    "        elif target_type == 'sia':\n",
    "            return '.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ef61744",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:59:15.558778Z",
     "start_time": "2021-06-17T06:59:15.556986Z"
    }
   },
   "outputs": [],
   "source": [
    "data_root = '/home/aiffel-dj10/SIAiffel_JJeonda/SIAiffel/datasets/data/SIA/buildings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d58db1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:59:15.566230Z",
     "start_time": "2021-06-17T06:59:15.559989Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset(opts):\n",
    "    \"\"\" Dataset And Augmentation\n",
    "    \"\"\"\n",
    "    if opts == 'cityscapes':\n",
    "        train_transform = ExtCompose([\n",
    "            #et.ExtResize( 512 ),\n",
    "            ExtRandomCrop(size=(512, 512)),\n",
    "            ExtColorJitter( brightness=0.5, contrast=0.5, saturation=0.5 ),\n",
    "            ExtRandomHorizontalFlip(),\n",
    "            ExtToTensor(),\n",
    "            ExtNormalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        val_transform = ExtCompose([\n",
    "            #et.ExtResize( 512 ),\n",
    "            ExtToTensor(),\n",
    "            ExtNormalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        train_dst = Cityscapes(root=data_root,\n",
    "                               split='train', transform=train_transform)\n",
    "        val_dst = Cityscapes(root=data_root,\n",
    "                             split='val', transform=val_transform)\n",
    "#trnasforms  \n",
    "#satellites  \n",
    "    if opts == 'satellites':\n",
    "        train_transform = ExtCompose([\n",
    "            #et.ExtResize( 512 ),\n",
    "            ExtRandomCrop(size=(512, 512)),\n",
    "            ExtColorJitter( brightness=0.5, contrast=0.5, saturation=0.5 ),\n",
    "            ExtRandomHorizontalFlip(),\n",
    "            ExtToTensor(),\n",
    "            ExtNormalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        val_transform = ExtCompose([\n",
    "            #et.ExtResize( 512 ),\n",
    "            ExtToTensor(),\n",
    "            ExtNormalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "#datasets\n",
    "        train_dst = Satellites(root=data_root,\n",
    "                               split='train', transform=train_transform)\n",
    "        val_dst = Satellites(root=data_root,\n",
    "                             split='val', transform=val_transform)\n",
    "    return train_dst, val_dst\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a55c957",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:59:15.574873Z",
     "start_time": "2021-06-17T06:59:15.567489Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dst, val_dst = get_dataset('satellites')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dst, batch_size=8, \n",
    "                                           shuffle=True, num_workers=2)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(val_dst, batch_size=8,\n",
    "                                        shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3324c32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:59:15.578835Z",
     "start_time": "2021-06-17T06:59:15.575970Z"
    }
   },
   "outputs": [],
   "source": [
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "963ab243",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:59:15.581634Z",
     "start_time": "2021-06-17T06:59:15.579950Z"
    }
   },
   "outputs": [],
   "source": [
    "# images, label = next(iter(train_loader))\n",
    "# images.shape, label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd6555a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:59:18.213795Z",
     "start_time": "2021-06-17T06:59:15.582786Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#    \n",
    "# dataiter = iter(get_dataset) #  \n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "images = torch.Tensor(images)\n",
    "# labels = torch.Tensor(labels)\n",
    "\n",
    "#   . #     \n",
    "img_grid = torchvision.utils.make_grid(images) # tensor or list\n",
    "\n",
    "#  .\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "\n",
    "# tensorboard . #  \n",
    "writer.add_image('images', img_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54210270",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:59:20.621037Z",
     "start_time": "2021-06-17T06:59:18.215038Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torchvision.transforms.functional' has no attribute 'interpolate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-b8e198865652>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplab/lib/python3.7/site-packages/torch/utils/tensorboard/writer.py\u001b[0m in \u001b[0;36madd_graph\u001b[0;34m(self, model, input_to_model, verbose)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'forward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m             \u001b[0;31m# A valid PyTorch model should have a 'forward' method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_file_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_to_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0;31m# Caffe2 models do not have the 'forward' method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplab/lib/python3.7/site-packages/torch/utils/tensorboard/_pytorch_graph.py\u001b[0m in \u001b[0;36mgraph\u001b[0;34m(model, args, verbose)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_model_mode_for_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainingMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEVAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# TODO: move outside of torch.onnx?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_inline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplab/lib/python3.7/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    740\u001b[0m             \u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m             \u001b[0m_force_outplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m             \u001b[0m_module_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m         )\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplab/lib/python3.7/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    938\u001b[0m                 \u001b[0mvar_lookup_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m                 \u001b[0m_force_outplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m             )\n\u001b[1;32m    942\u001b[0m             \u001b[0mcheck_trace_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplab/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplab/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-038fbcc30085>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplab/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplab/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-69c644393fd8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, feature)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mlow_level_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'low_level'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0moutput_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maspp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0moutput_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlow_level_feature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mlow_level_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_feature\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplab/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplab/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-69c644393fd8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplab/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplab/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-69c644393fd8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mASPPPooling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mASPP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torchvision.transforms.functional' has no attribute 'interpolate'"
     ]
    }
   ],
   "source": [
    "#\n",
    "writer.add_graph(model_1, images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823baf06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644a47da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938e1115",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplab",
   "language": "python",
   "name": "deeplab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
