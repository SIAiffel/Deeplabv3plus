{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchvision.models.utils import load_state_dict_from_url\n",
    "import torch.nn.functional as F\n",
    "\n",
    "__all__ = ['MobileNetV2', 'mobilenet_v2']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'mobilenet_v2': 'https://download.pytorch.org/models/mobilenet_v2-b0353104.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "class ConvBNReLU(nn.Sequential):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, dilation=1, groups=1):\n",
    "        #padding = (kernel_size - 1) // 2\n",
    "        super(ConvBNReLU, self).__init__(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, 0, dilation=dilation, groups=groups, bias=False),\n",
    "            nn.BatchNorm2d(out_planes),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "def fixed_padding(kernel_size, dilation):\n",
    "    kernel_size_effective = kernel_size + (kernel_size - 1) * (dilation - 1)\n",
    "    pad_total = kernel_size_effective - 1\n",
    "    pad_beg = pad_total // 2\n",
    "    pad_end = pad_total - pad_beg\n",
    "    return (pad_beg, pad_end, pad_beg, pad_end) \n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, dilation, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = int(round(inp * expand_ratio))\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        layers = []\n",
    "        if expand_ratio != 1:\n",
    "            # pw\n",
    "            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n",
    "\n",
    "        layers.extend([\n",
    "            # dw\n",
    "            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, dilation=dilation, groups=hidden_dim),\n",
    "            # pw-linear\n",
    "            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(oup),\n",
    "        ])\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "        self.input_padding = fixed_padding( 3, dilation )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_pad = F.pad(x, self.input_padding)\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x_pad)\n",
    "        else:\n",
    "            return self.conv(x_pad)\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=1000, output_stride=8, width_mult=1.0, inverted_residual_setting=None, round_nearest=8):\n",
    "        \"\"\"\n",
    "        MobileNet V2 main class\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): Number of classes\n",
    "            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n",
    "            inverted_residual_setting: Network structure\n",
    "            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n",
    "            Set to 1 to turn off rounding\n",
    "        \"\"\"\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "        self.output_stride = output_stride\n",
    "        current_stride = 1\n",
    "        if inverted_residual_setting is None:\n",
    "            inverted_residual_setting = [\n",
    "                # t, c, n, s\n",
    "                [1, 16, 1, 1],\n",
    "                [6, 24, 2, 2],\n",
    "                [6, 32, 3, 2],\n",
    "                [6, 64, 4, 2],\n",
    "                [6, 96, 3, 1],\n",
    "                [6, 160, 3, 2],\n",
    "                [6, 320, 1, 1],\n",
    "            ]\n",
    "\n",
    "        # only check the first element, assuming user knows t,c,n,s are required\n",
    "        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n",
    "            raise ValueError(\"inverted_residual_setting should be non-empty \"\n",
    "                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n",
    "        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n",
    "        features = [ConvBNReLU(3, input_channel, stride=2)]\n",
    "        current_stride *= 2\n",
    "        dilation=1\n",
    "        previous_dilation = 1\n",
    "\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in inverted_residual_setting:\n",
    "            output_channel = _make_divisible(c * width_mult, round_nearest)\n",
    "            previous_dilation = dilation\n",
    "            if current_stride == output_stride:\n",
    "                stride = 1\n",
    "                dilation *= s\n",
    "            else:\n",
    "                stride = s\n",
    "                current_stride *= s\n",
    "            output_channel = int(c * width_mult)\n",
    "\n",
    "            for i in range(n):\n",
    "                if i==0:\n",
    "                    features.append(block(input_channel, output_channel, stride, previous_dilation, expand_ratio=t))\n",
    "                else:\n",
    "                    features.append(block(input_channel, output_channel, 1, dilation, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*features)\n",
    "\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, num_classes),\n",
    "        )\n",
    "\n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.mean([2, 3])\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def mobilenet_v2(pretrained=False, progress=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a MobileNetV2 architecture from\n",
    "    `\"MobileNetV2: Inverted Residuals and Linear Bottlenecks\" <https://arxiv.org/abs/1801.04381>`_.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    model = MobileNetV2(**kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls['mobilenet_v2'],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "class _SimpleSegmentationModel(nn.Module):\n",
    "    def __init__(self, backbone, classifier):\n",
    "        super(_SimpleSegmentationModel, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.classifier = classifier\n",
    "        \n",
    "    def forward(self, x):\n",
    "        input_shape = x.shape[-2:]\n",
    "        print()\n",
    "        features = self.backbone(x)\n",
    "        x = self.classifier(features)\n",
    "        x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IntermediateLayerGetter(nn.ModuleDict):\n",
    "    \"\"\"\n",
    "    Module wrapper that returns intermediate layers from a model\n",
    "\n",
    "    It has a strong assumption that the modules have been registered\n",
    "    into the model in the same order as they are used.\n",
    "    This means that one should **not** reuse the same nn.Module\n",
    "    twice in the forward if you want this to work.\n",
    "\n",
    "    Additionally, it is only able to query submodules that are directly\n",
    "    assigned to the model. So if `model` is passed, `model.feature1` can\n",
    "    be returned, but not `model.feature1.layer2`.\n",
    "\n",
    "    Arguments:\n",
    "        model (nn.Module): model on which we will extract the features\n",
    "        return_layers (Dict[name, new_name]): a dict containing the names\n",
    "            of the modules for which the activations will be returned as\n",
    "            the key of the dict, and the value of the dict is the name\n",
    "            of the returned activation (which the user can specify).\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> m = torchvision.models.resnet18(pretrained=True)\n",
    "        >>> # extract layer1 and layer3, giving as names `feat1` and feat2`\n",
    "        >>> new_m = torchvision.models._utils.IntermediateLayerGetter(m,\n",
    "        >>>     {'layer1': 'feat1', 'layer3': 'feat2'})\n",
    "        >>> out = new_m(torch.rand(1, 3, 224, 224))\n",
    "        >>> print([(k, v.shape) for k, v in out.items()])\n",
    "        >>>     [('feat1', torch.Size([1, 64, 56, 56])),\n",
    "        >>>      ('feat2', torch.Size([1, 256, 14, 14]))]\n",
    "    \"\"\"\n",
    "    def __init__(self, model, return_layers):\n",
    "        if not set(return_layers).issubset([name for name, _ in model.named_children()]):\n",
    "            raise ValueError(\"return_layers are not present in model\")\n",
    "\n",
    "        orig_return_layers = return_layers\n",
    "        return_layers = {k: v for k, v in return_layers.items()}\n",
    "        layers = OrderedDict()\n",
    "        for name, module in model.named_children():\n",
    "            layers[name] = module\n",
    "            if name in return_layers:\n",
    "                del return_layers[name]\n",
    "            if not return_layers:\n",
    "                break\n",
    "\n",
    "        super(IntermediateLayerGetter, self).__init__(layers)\n",
    "        self.return_layers = orig_return_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = OrderedDict()\n",
    "        for name, module in self.named_children():\n",
    "            x = module(x)\n",
    "            if name in self.return_layers:\n",
    "                out_name = self.return_layers[name]\n",
    "                out[out_name] = x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#from .utils import _SimpleSegmentationModel\n",
    "\n",
    "\n",
    "__all__ = [\"DeepLabV3\"]\n",
    "\n",
    "\n",
    "class DeepLabV3(_SimpleSegmentationModel):\n",
    "    \"\"\"\n",
    "    Implements DeepLabV3 model from\n",
    "    `\"Rethinking Atrous Convolution for Semantic Image Segmentation\"\n",
    "    <https://arxiv.org/abs/1706.05587>`_.\n",
    "\n",
    "    Arguments:\n",
    "        backbone (nn.Module): the network used to compute the features for the model.\n",
    "            The backbone should return an OrderedDict[Tensor], with the key being\n",
    "            \"out\" for the last feature map used, and \"aux\" if an auxiliary classifier\n",
    "            is used.\n",
    "        classifier (nn.Module): module that takes the \"out\" element returned from\n",
    "            the backbone and returns a dense prediction.\n",
    "        aux_classifier (nn.Module, optional): auxiliary classifier used during training\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "class DeepLabHeadV3Plus(nn.Module):\n",
    "    def __init__(self, in_channels, low_level_channels, num_classes, aspp_dilate=[12, 24, 36]):\n",
    "        super(DeepLabHeadV3Plus, self).__init__()\n",
    "        self.project = nn.Sequential( \n",
    "            nn.Conv2d(low_level_channels, 48, 1, bias=False),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.aspp = ASPP(in_channels, aspp_dilate)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(304, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, num_classes, 1)\n",
    "        )\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, feature):\n",
    "        low_level_feature = self.project( feature['low_level'] )\n",
    "        output_feature = self.aspp(feature['out'])\n",
    "        output_feature = F.interpolate(output_feature, size=low_level_feature.shape[2:], mode='bilinear', align_corners=False)\n",
    "        return self.classifier( torch.cat( [ low_level_feature, output_feature ], dim=1 ) )\n",
    "    \n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "class DeepLabHead(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, aspp_dilate=[12, 24, 36]):\n",
    "        super(DeepLabHead, self).__init__()\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            ASPP(in_channels, aspp_dilate),\n",
    "            nn.Conv2d(256, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, num_classes, 1)\n",
    "        )\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, feature):\n",
    "        return self.classifier( feature['out'] )\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "class AtrousSeparableConvolution(nn.Module):\n",
    "    \"\"\" Atrous Separable Convolution\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                            stride=1, padding=0, dilation=1, bias=True):\n",
    "        super(AtrousSeparableConvolution, self).__init__()\n",
    "        self.body = nn.Sequential(\n",
    "            # Separable Conv\n",
    "            nn.Conv2d( in_channels, in_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias, groups=in_channels ),\n",
    "            # PointWise Conv\n",
    "            nn.Conv2d( in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=bias),\n",
    "        )\n",
    "        \n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "class ASPPConv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, dilation):\n",
    "        modules = [\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        super(ASPPConv, self).__init__(*modules)\n",
    "\n",
    "class ASPPPooling(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ASPPPooling, self).__init__(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.shape[-2:]\n",
    "        x = super(ASPPPooling, self).forward(x)\n",
    "        return F.interpolate(x, size=size, mode='bilinear', align_corners=False)\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, atrous_rates):\n",
    "        super(ASPP, self).__init__()\n",
    "        out_channels = 256\n",
    "        modules = []\n",
    "        modules.append(nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)))\n",
    "\n",
    "        rate1, rate2, rate3 = tuple(atrous_rates)\n",
    "        modules.append(ASPPConv(in_channels, out_channels, rate1))\n",
    "        modules.append(ASPPConv(in_channels, out_channels, rate2))\n",
    "        modules.append(ASPPConv(in_channels, out_channels, rate3))\n",
    "        modules.append(ASPPPooling(in_channels, out_channels))\n",
    "\n",
    "        self.convs = nn.ModuleList(modules)\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(5 * out_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = []\n",
    "        for conv in self.convs:\n",
    "            res.append(conv(x))\n",
    "        res = torch.cat(res, dim=1)\n",
    "        return self.project(res)\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_separable_conv(module):\n",
    "    new_module = module\n",
    "    if isinstance(module, nn.Conv2d) and module.kernel_size[0]>1:\n",
    "        new_module = AtrousSeparableConvolution(module.in_channels,\n",
    "                                      module.out_channels, \n",
    "                                      module.kernel_size,\n",
    "                                      module.stride,\n",
    "                                      module.padding,\n",
    "                                      module.dilation,\n",
    "                                      module.bias)\n",
    "    for name, child in module.named_children():\n",
    "        new_module.add_module(name, convert_to_separable_conv(child))\n",
    "    return new_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    from .utils import IntermediateLayerGetter\n",
    "#    from ._deeplab import DeepLabHead, DeepLabHeadV3Plus, DeepLabV3\n",
    "#    from .backbone import resnet\n",
    "#    from .backbone import mobilenetv2\n",
    "\n",
    "def _segm_resnet(name, backbone_name, num_classes, output_stride, pretrained_backbone):\n",
    "\n",
    "    if output_stride==8:\n",
    "        replace_stride_with_dilation=[False, True, True]\n",
    "        aspp_dilate = [12, 24, 36]\n",
    "    else:\n",
    "        replace_stride_with_dilation=[False, False, True]\n",
    "        aspp_dilate = [6, 12, 18]\n",
    "\n",
    "    backbone = resnet.__dict__[backbone_name](\n",
    "        pretrained=pretrained_backbone,\n",
    "        replace_stride_with_dilation=replace_stride_with_dilation)\n",
    "    \n",
    "    inplanes = 2048\n",
    "    low_level_planes = 256\n",
    "\n",
    "    if name=='deeplabv3plus':\n",
    "        return_layers = {'layer4': 'out', 'layer1': 'low_level'}\n",
    "        classifier = DeepLabHeadV3Plus(inplanes, low_level_planes, num_classes, aspp_dilate)\n",
    "    elif name=='deeplabv3':\n",
    "        return_layers = {'layer4': 'out'}\n",
    "        classifier = DeepLabHead(inplanes , num_classes, aspp_dilate)\n",
    "    backbone = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
    "\n",
    "    model = DeepLabV3(backbone, classifier)\n",
    "    return model\n",
    "\n",
    "def _segm_mobilenet(name, backbone_name, num_classes, output_stride, pretrained_backbone):\n",
    "    if output_stride==8:\n",
    "        aspp_dilate = [12, 24, 36]\n",
    "    else:\n",
    "        aspp_dilate = [6, 12, 18]\n",
    "\n",
    "    backbone = mobilenet_v2(pretrained=pretrained_backbone, output_stride=output_stride)\n",
    "    \n",
    "    # rename layers\n",
    "    backbone.low_level_features = backbone.features[0:4]\n",
    "    backbone.high_level_features = backbone.features[4:-1]\n",
    "    backbone.features = None\n",
    "    backbone.classifier = None\n",
    "\n",
    "    inplanes = 320\n",
    "    low_level_planes = 24\n",
    "    \n",
    "    if name=='deeplabv3plus':\n",
    "        return_layers = {'high_level_features': 'out', 'low_level_features': 'low_level'}\n",
    "        classifier = DeepLabHeadV3Plus(inplanes, low_level_planes, num_classes, aspp_dilate)\n",
    "    elif name=='deeplabv3':\n",
    "        return_layers = {'high_level_features': 'out'}\n",
    "        classifier = DeepLabHead(inplanes , num_classes, aspp_dilate)\n",
    "    backbone = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
    "\n",
    "    model = DeepLabV3(backbone, classifier)\n",
    "    return model\n",
    "\n",
    "def _load_model(arch_type, backbone, num_classes, output_stride, pretrained_backbone):\n",
    "\n",
    "    if backbone=='mobilenetv2':\n",
    "        model = _segm_mobilenet(arch_type, backbone, num_classes, output_stride=output_stride, pretrained_backbone=pretrained_backbone)\n",
    "    elif backbone.startswith('resnet'):\n",
    "        model = _segm_resnet(arch_type, backbone, num_classes, output_stride=output_stride, pretrained_backbone=pretrained_backbone)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return model\n",
    "\n",
    "\n",
    "# Deeplab v3\n",
    "\n",
    "def deeplabv3_resnet50(num_classes=21, output_stride=8, pretrained_backbone=True):\n",
    "    \"\"\"Constructs a DeepLabV3 model with a ResNet-50 backbone.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        output_stride (int): output stride for deeplab.\n",
    "        pretrained_backbone (bool): If True, use the pretrained backbone.\n",
    "    \"\"\"\n",
    "    return _load_model('deeplabv3', 'resnet50', num_classes, output_stride=output_stride, pretrained_backbone=pretrained_backbone)\n",
    "\n",
    "def deeplabv3_resnet101(num_classes=21, output_stride=8, pretrained_backbone=True):\n",
    "    \"\"\"Constructs a DeepLabV3 model with a ResNet-101 backbone.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        output_stride (int): output stride for deeplab.\n",
    "        pretrained_backbone (bool): If True, use the pretrained backbone.\n",
    "    \"\"\"\n",
    "    return _load_model('deeplabv3', 'resnet101', num_classes, output_stride=output_stride, pretrained_backbone=pretrained_backbone)\n",
    "\n",
    "def deeplabv3_mobilenet(num_classes=21, output_stride=8, pretrained_backbone=True, **kwargs):\n",
    "    \"\"\"Constructs a DeepLabV3 model with a MobileNetv2 backbone.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        output_stride (int): output stride for deeplab.\n",
    "        pretrained_backbone (bool): If True, use the pretrained backbone.\n",
    "    \"\"\"\n",
    "    return _load_model('deeplabv3', 'mobilenetv2', num_classes, output_stride=output_stride, pretrained_backbone=pretrained_backbone)\n",
    "\n",
    "\n",
    "# Deeplab v3+\n",
    "\n",
    "def deeplabv3plus_resnet50(num_classes=21, output_stride=8, pretrained_backbone=True):\n",
    "    \"\"\"Constructs a DeepLabV3 model with a ResNet-50 backbone.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        output_stride (int): output stride for deeplab.\n",
    "        pretrained_backbone (bool): If True, use the pretrained backbone.\n",
    "    \"\"\"\n",
    "    return _load_model('deeplabv3plus', 'resnet50', num_classes, output_stride=output_stride, pretrained_backbone=pretrained_backbone)\n",
    "\n",
    "\n",
    "def deeplabv3plus_resnet101(num_classes=21, output_stride=8, pretrained_backbone=True):\n",
    "    \"\"\"Constructs a DeepLabV3+ model with a ResNet-101 backbone.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        output_stride (int): output stride for deeplab.\n",
    "        pretrained_backbone (bool): If True, use the pretrained backbone.\n",
    "    \"\"\"\n",
    "    return _load_model('deeplabv3plus', 'resnet101', num_classes, output_stride=output_stride, pretrained_backbone=pretrained_backbone)\n",
    "\n",
    "\n",
    "def deeplabv3plus_mobilenet(num_classes=21, output_stride=8, pretrained_backbone=True):\n",
    "    \"\"\"Constructs a DeepLabV3+ model with a MobileNetv2 backbone.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        output_stride (int): output stride for deeplab.\n",
    "        pretrained_backbone (bool): If True, use the pretrained backbone.\n",
    "    \"\"\"\n",
    "    return _load_model('deeplabv3plus', 'mobilenetv2', num_classes, output_stride=output_stride, pretrained_backbone=pretrained_backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import network\n",
    "model_map = {\n",
    "        'deeplabv3_resnet50': deeplabv3_resnet50,\n",
    "        'deeplabv3plus_resnet50': deeplabv3plus_resnet50,\n",
    "        'deeplabv3_resnet101': deeplabv3_resnet101,\n",
    "        'deeplabv3plus_resnet101': deeplabv3plus_resnet101,\n",
    "        'deeplabv3_mobilenet': deeplabv3_mobilenet,\n",
    "        'deeplabv3plus_mobilenet': deeplabv3plus_mobilenet\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKPT_PATH = '/home/aiffel-dj17/aiffel/siaiffel/DeepLabV3Plus-Pytorch-master/checkpoints/issac/best_deeplabv3plus_mobilenet_satellites_os16.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(CKPT_PATH, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "x = torch.randn(1, 3, 256, 256).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = model_map['deeplabv3plus_mobilenet'](num_classes=19, output_stride=16)\n",
    "#model.load_state_dict(checkpoint[\"model_state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepLabHeadV3Plus(\n",
       "  (project): Sequential(\n",
       "    (0): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (aspp): ASPP(\n",
       "    (convs): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): ASPPConv(\n",
       "        (0): AtrousSeparableConvolution(\n",
       "          (body): Sequential(\n",
       "            (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), groups=320, bias=False)\n",
       "            (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): ASPPConv(\n",
       "        (0): AtrousSeparableConvolution(\n",
       "          (body): Sequential(\n",
       "            (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), groups=320, bias=False)\n",
       "            (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): ASPPConv(\n",
       "        (0): AtrousSeparableConvolution(\n",
       "          (body): Sequential(\n",
       "            (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18), groups=320, bias=False)\n",
       "            (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): ASPPPooling(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (project): Sequential(\n",
       "      (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): AtrousSeparableConvolution(\n",
       "      (body): Sequential(\n",
       "        (0): Conv2d(304, 304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=304, bias=False)\n",
       "        (1): Conv2d(304, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(256, 19, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_map['deeplabv3plus_mobilenet'](num_classes=19, output_stride=16)\n",
    "convert_to_separable_conv(model.classifier)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(checkpoint[\"model_state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepLabV3(\n",
       "  (backbone): IntermediateLayerGetter(\n",
       "    (low_level_features): Sequential(\n",
       "      (0): ConvBNReLU(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace=True)\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), groups=96, bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (high_level_features): Sequential(\n",
       "      (4): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (7): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (8): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (9): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (10): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (11): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (12): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (13): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (14): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (15): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (16): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (17): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): DeepLabHeadV3Plus(\n",
       "    (project): Sequential(\n",
       "      (0): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (aspp): ASPP(\n",
       "      (convs): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): ASPPConv(\n",
       "          (0): AtrousSeparableConvolution(\n",
       "            (body): Sequential(\n",
       "              (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), groups=320, bias=False)\n",
       "              (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): ASPPConv(\n",
       "          (0): AtrousSeparableConvolution(\n",
       "            (body): Sequential(\n",
       "              (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), groups=320, bias=False)\n",
       "              (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): ASPPConv(\n",
       "          (0): AtrousSeparableConvolution(\n",
       "            (body): Sequential(\n",
       "              (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18), groups=320, bias=False)\n",
       "              (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): ASPPPooling(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (project): Sequential(\n",
       "        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (classifier): Sequential(\n",
       "      (0): AtrousSeparableConvolution(\n",
       "        (body): Sequential(\n",
       "          (0): Conv2d(304, 304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=304, bias=False)\n",
       "          (1): Conv2d(304, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(256, 19, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepLabV3(\n",
       "  (backbone): IntermediateLayerGetter(\n",
       "    (low_level_features): Sequential(\n",
       "      (0): ConvBNReLU(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace=True)\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), groups=96, bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (high_level_features): Sequential(\n",
       "      (4): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (7): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (8): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (9): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (10): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (11): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (12): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (13): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (14): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (15): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (16): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (17): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): DeepLabHeadV3Plus(\n",
       "    (project): Sequential(\n",
       "      (0): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (aspp): ASPP(\n",
       "      (convs): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): ASPPConv(\n",
       "          (0): AtrousSeparableConvolution(\n",
       "            (body): Sequential(\n",
       "              (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), groups=320, bias=False)\n",
       "              (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): ASPPConv(\n",
       "          (0): AtrousSeparableConvolution(\n",
       "            (body): Sequential(\n",
       "              (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), groups=320, bias=False)\n",
       "              (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): ASPPConv(\n",
       "          (0): AtrousSeparableConvolution(\n",
       "            (body): Sequential(\n",
       "              (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18), groups=320, bias=False)\n",
       "              (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): ASPPPooling(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (project): Sequential(\n",
       "        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (classifier): Sequential(\n",
       "      (0): AtrousSeparableConvolution(\n",
       "        (body): Sequential(\n",
       "          (0): Conv2d(304, 304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=304, bias=False)\n",
       "          (1): Conv2d(304, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(256, 19, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "import sys\n",
    "import os \n",
    "\n",
    "onnx_model_path = './satellitesopencv1_good.onnx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 3, 256, 256).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/onnx/symbolic_helper.py:347: UserWarning: You are trying to export the model with onnx:Upsample for ONNX opset version 9. This operator might cause results to not match the expected results by PyTorch.\n",
      "ONNX's Upsample/Resize operator did not match Pytorch's Interpolation until opset 11. Attributes to determine how to transform the input were added in onnx:Resize in opset 11 to support Pytorch's behavior (like coordinate_transformation_mode and nearest_mode).\n",
      "We recommend using opset 11 and above for models using this operator. \n",
      "  \"\" + str(_export_onnx_opset_version) + \". \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%input.1 : Float(1, 3, 256, 256, strides=[196608, 65536, 256, 1], requires_grad=0, device=cuda:0),\n",
      "      %classifier.aspp.convs.1.0.body.0.weight : Float(320, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cuda:0),\n",
      "      %classifier.aspp.convs.2.0.body.0.weight : Float(320, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cuda:0),\n",
      "      %classifier.aspp.convs.3.0.body.0.weight : Float(320, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cuda:0),\n",
      "      %classifier.classifier.0.body.0.weight : Float(304, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cuda:0),\n",
      "      %classifier.classifier.3.weight : Float(19, 256, 1, 1, strides=[256, 1, 1, 1], requires_grad=1, device=cuda:0),\n",
      "      %classifier.classifier.3.bias : Float(19, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %608 : Float(32, 3, 3, 3, strides=[27, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %609 : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %611 : Float(32, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %612 : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %614 : Float(16, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %615 : Float(16, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %617 : Float(96, 16, 1, 1, strides=[16, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %618 : Float(96, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %620 : Float(96, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %621 : Float(96, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %623 : Float(24, 96, 1, 1, strides=[96, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %624 : Float(24, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %626 : Float(144, 24, 1, 1, strides=[24, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %627 : Float(144, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %629 : Float(144, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %630 : Float(144, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %632 : Float(24, 144, 1, 1, strides=[144, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %633 : Float(24, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %635 : Float(144, 24, 1, 1, strides=[24, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %636 : Float(144, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %638 : Float(144, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %639 : Float(144, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %641 : Float(32, 144, 1, 1, strides=[144, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %642 : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %644 : Float(192, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %645 : Float(192, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %647 : Float(192, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %648 : Float(192, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %650 : Float(32, 192, 1, 1, strides=[192, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %651 : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %653 : Float(192, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %654 : Float(192, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %656 : Float(192, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %657 : Float(192, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %659 : Float(32, 192, 1, 1, strides=[192, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %660 : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %662 : Float(192, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %663 : Float(192, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %665 : Float(192, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %666 : Float(192, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %668 : Float(64, 192, 1, 1, strides=[192, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %669 : Float(64, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %671 : Float(384, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %672 : Float(384, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %674 : Float(384, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %675 : Float(384, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %677 : Float(64, 384, 1, 1, strides=[384, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %678 : Float(64, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %680 : Float(384, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %681 : Float(384, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %683 : Float(384, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %684 : Float(384, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %686 : Float(64, 384, 1, 1, strides=[384, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %687 : Float(64, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %689 : Float(384, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %690 : Float(384, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %692 : Float(384, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %693 : Float(384, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %695 : Float(64, 384, 1, 1, strides=[384, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %696 : Float(64, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %698 : Float(384, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %699 : Float(384, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %701 : Float(384, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %702 : Float(384, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %704 : Float(96, 384, 1, 1, strides=[384, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %705 : Float(96, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %707 : Float(576, 96, 1, 1, strides=[96, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %708 : Float(576, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %710 : Float(576, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %711 : Float(576, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %713 : Float(96, 576, 1, 1, strides=[576, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %714 : Float(96, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %716 : Float(576, 96, 1, 1, strides=[96, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %717 : Float(576, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %719 : Float(576, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %720 : Float(576, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %722 : Float(96, 576, 1, 1, strides=[576, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %723 : Float(96, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %725 : Float(576, 96, 1, 1, strides=[96, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %726 : Float(576, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %728 : Float(576, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %729 : Float(576, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %731 : Float(160, 576, 1, 1, strides=[576, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %732 : Float(160, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %734 : Float(960, 160, 1, 1, strides=[160, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %735 : Float(960, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %737 : Float(960, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %738 : Float(960, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %740 : Float(160, 960, 1, 1, strides=[960, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %741 : Float(160, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %743 : Float(960, 160, 1, 1, strides=[160, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %744 : Float(960, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %746 : Float(960, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %747 : Float(960, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %749 : Float(160, 960, 1, 1, strides=[960, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %750 : Float(160, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %752 : Float(960, 160, 1, 1, strides=[160, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %753 : Float(960, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %755 : Float(960, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cuda:0),\n",
      "      %756 : Float(960, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %758 : Float(320, 960, 1, 1, strides=[960, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %759 : Float(320, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %761 : Float(48, 24, 1, 1, strides=[24, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %762 : Float(48, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %764 : Float(256, 320, 1, 1, strides=[320, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %765 : Float(256, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %767 : Float(256, 320, 1, 1, strides=[320, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %768 : Float(256, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %770 : Float(256, 320, 1, 1, strides=[320, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %771 : Float(256, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %773 : Float(256, 320, 1, 1, strides=[320, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %774 : Float(256, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %776 : Float(256, 320, 1, 1, strides=[320, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %777 : Float(256, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %779 : Float(256, 1280, 1, 1, strides=[1280, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %780 : Float(256, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %782 : Float(256, 304, 1, 1, strides=[304, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %783 : Float(256, strides=[1], requires_grad=0, device=cuda:0)):\n",
      "  %361 : Long(4, strides=[1], device=cpu) = onnx::Shape(%input.1)\n",
      "  %362 : Long(device=cpu) = onnx::Constant[value={2}]()\n",
      "  %363 : Long(device=cpu) = onnx::Gather[axis=0](%361, %362) # <ipython-input-3-038fbcc30085>:14:0\n",
      "  %364 : Long(4, strides=[1], device=cpu) = onnx::Shape(%input.1)\n",
      "  %365 : Long(device=cpu) = onnx::Constant[value={3}]()\n",
      "  %366 : Long(device=cpu) = onnx::Gather[axis=0](%364, %365) # <ipython-input-3-038fbcc30085>:14:0\n",
      "  %607 : Float(1, 32, 127, 127, strides=[516128, 16129, 127, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%input.1, %608, %609)\n",
      "  %369 : Float(1, 32, 127, 127, strides=[516128, 16129, 127, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%607) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %370 : Float(1, 32, 129, 129, strides=[532512, 16641, 129, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%369) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:4000:0\n",
      "  %610 : Float(1, 32, 127, 127, strides=[516128, 16129, 127, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=32, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%370, %611, %612)\n",
      "  %373 : Float(1, 32, 127, 127, strides=[516128, 16129, 127, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%610) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %613 : Float(1, 16, 127, 127, strides=[258064, 16129, 127, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%373, %614, %615)\n",
      "  %376 : Float(1, 16, 129, 129, strides=[266256, 16641, 129, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%613) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:4000:0\n",
      "  %616 : Float(1, 96, 129, 129, strides=[1597536, 16641, 129, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%376, %617, %618)\n",
      "  %379 : Float(1, 96, 129, 129, strides=[1597536, 16641, 129, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%616) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %619 : Float(1, 96, 64, 64, strides=[393216, 4096, 64, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=96, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%379, %620, %621)\n",
      "  %382 : Float(1, 96, 64, 64, strides=[393216, 4096, 64, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%619) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %622 : Float(1, 24, 64, 64, strides=[98304, 4096, 64, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%382, %623, %624)\n",
      "  %385 : Float(1, 24, 66, 66, strides=[104544, 4356, 66, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%622) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:4000:0\n",
      "  %625 : Float(1, 144, 66, 66, strides=[627264, 4356, 66, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%385, %626, %627)\n",
      "  %388 : Float(1, 144, 66, 66, strides=[627264, 4356, 66, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%625) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %628 : Float(1, 144, 64, 64, strides=[589824, 4096, 64, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=144, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%388, %629, %630)\n",
      "  %391 : Float(1, 144, 64, 64, strides=[589824, 4096, 64, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%628) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %631 : Float(1, 24, 64, 64, strides=[98304, 4096, 64, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%391, %632, %633)\n",
      "  %394 : Float(1, 24, 64, 64, strides=[98304, 4096, 64, 1], requires_grad=1, device=cuda:0) = onnx::Add(%622, %631) # <ipython-input-2-a4271324998c>:77:0\n",
      "  %395 : Float(1, 24, 66, 66, strides=[104544, 4356, 66, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%394) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:4000:0\n",
      "  %634 : Float(1, 144, 66, 66, strides=[627264, 4356, 66, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%395, %635, %636)\n",
      "  %398 : Float(1, 144, 66, 66, strides=[627264, 4356, 66, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%634) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %637 : Float(1, 144, 32, 32, strides=[147456, 1024, 32, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=144, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%398, %638, %639)\n",
      "  %401 : Float(1, 144, 32, 32, strides=[147456, 1024, 32, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%637) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %640 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%401, %641, %642)\n",
      "  %404 : Float(1, 32, 34, 34, strides=[36992, 1156, 34, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%640) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:4000:0\n",
      "  %643 : Float(1, 192, 34, 34, strides=[221952, 1156, 34, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%404, %644, %645)\n",
      "  %407 : Float(1, 192, 34, 34, strides=[221952, 1156, 34, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%643) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %646 : Float(1, 192, 32, 32, strides=[196608, 1024, 32, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=192, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%407, %647, %648)\n",
      "  %410 : Float(1, 192, 32, 32, strides=[196608, 1024, 32, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%646) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %649 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%410, %650, %651)\n",
      "  %413 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cuda:0) = onnx::Add(%640, %649) # <ipython-input-2-a4271324998c>:77:0\n",
      "  %414 : Float(1, 32, 34, 34, strides=[36992, 1156, 34, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%413) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:4000:0\n",
      "  %652 : Float(1, 192, 34, 34, strides=[221952, 1156, 34, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%414, %653, %654)\n",
      "  %417 : Float(1, 192, 34, 34, strides=[221952, 1156, 34, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%652) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %655 : Float(1, 192, 32, 32, strides=[196608, 1024, 32, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=192, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%417, %656, %657)\n",
      "  %420 : Float(1, 192, 32, 32, strides=[196608, 1024, 32, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%655) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %658 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%420, %659, %660)\n",
      "  %423 : Float(1, 32, 32, 32, strides=[32768, 1024, 32, 1], requires_grad=1, device=cuda:0) = onnx::Add(%413, %658) # <ipython-input-2-a4271324998c>:77:0\n",
      "  %424 : Float(1, 32, 34, 34, strides=[36992, 1156, 34, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%423) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:4000:0\n",
      "  %661 : Float(1, 192, 34, 34, strides=[221952, 1156, 34, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%424, %662, %663)\n",
      "  %427 : Float(1, 192, 34, 34, strides=[221952, 1156, 34, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%661) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %664 : Float(1, 192, 16, 16, strides=[49152, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=192, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%427, %665, %666)\n",
      "  %430 : Float(1, 192, 16, 16, strides=[49152, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%664) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %667 : Float(1, 64, 16, 16, strides=[16384, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%430, %668, %669)\n",
      "  %433 : Float(1, 64, 18, 18, strides=[20736, 324, 18, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%667) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:4000:0\n",
      "  %670 : Float(1, 384, 18, 18, strides=[124416, 324, 18, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%433, %671, %672)\n",
      "  %436 : Float(1, 384, 18, 18, strides=[124416, 324, 18, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%670) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %673 : Float(1, 384, 16, 16, strides=[98304, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%436, %674, %675)\n",
      "  %439 : Float(1, 384, 16, 16, strides=[98304, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%673) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %676 : Float(1, 64, 16, 16, strides=[16384, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%439, %677, %678)\n",
      "  %442 : Float(1, 64, 16, 16, strides=[16384, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Add(%667, %676) # <ipython-input-2-a4271324998c>:77:0\n",
      "  %443 : Float(1, 64, 18, 18, strides=[20736, 324, 18, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%442) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:4000:0\n",
      "  %679 : Float(1, 384, 18, 18, strides=[124416, 324, 18, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%443, %680, %681)\n",
      "  %446 : Float(1, 384, 18, 18, strides=[124416, 324, 18, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%679) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %682 : Float(1, 384, 16, 16, strides=[98304, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%446, %683, %684)\n",
      "  %449 : Float(1, 384, 16, 16, strides=[98304, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%682) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %685 : Float(1, 64, 16, 16, strides=[16384, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%449, %686, %687)\n",
      "  %452 : Float(1, 64, 16, 16, strides=[16384, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Add(%442, %685) # <ipython-input-2-a4271324998c>:77:0\n",
      "  %453 : Float(1, 64, 18, 18, strides=[20736, 324, 18, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%452) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:4000:0\n",
      "  %688 : Float(1, 384, 18, 18, strides=[124416, 324, 18, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%453, %689, %690)\n",
      "  %456 : Float(1, 384, 18, 18, strides=[124416, 324, 18, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%688) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %691 : Float(1, 384, 16, 16, strides=[98304, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%456, %692, %693)\n",
      "  %459 : Float(1, 384, 16, 16, strides=[98304, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%691) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %694 : Float(1, 64, 16, 16, strides=[16384, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%459, %695, %696)\n",
      "  %462 : Float(1, 64, 16, 16, strides=[16384, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Add(%452, %694) # <ipython-input-2-a4271324998c>:77:0\n",
      "  %463 : Float(1, 64, 18, 18, strides=[20736, 324, 18, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%462) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:4000:0\n",
      "  %697 : Float(1, 384, 18, 18, strides=[124416, 324, 18, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%463, %698, %699)\n",
      "  %466 : Float(1, 384, 18, 18, strides=[124416, 324, 18, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%697) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %700 : Float(1, 384, 16, 16, strides=[98304, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%466, %701, %702)\n",
      "  %469 : Float(1, 384, 16, 16, strides=[98304, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%700) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %703 : Float(1, 96, 16, 16, strides=[24576, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%469, %704, %705)\n",
      "  %472 : Float(1, 96, 18, 18, strides=[31104, 324, 18, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%703) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:4000:0\n",
      "  %706 : Float(1, 576, 18, 18, strides=[186624, 324, 18, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%472, %707, %708)\n",
      "  %475 : Float(1, 576, 18, 18, strides=[186624, 324, 18, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%706) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %709 : Float(1, 576, 16, 16, strides=[147456, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=576, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%475, %710, %711)\n",
      "  %478 : Float(1, 576, 16, 16, strides=[147456, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%709) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %712 : Float(1, 96, 16, 16, strides=[24576, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%478, %713, %714)\n",
      "  %481 : Float(1, 96, 16, 16, strides=[24576, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Add(%703, %712) # <ipython-input-2-a4271324998c>:77:0\n",
      "  %482 : Float(1, 96, 18, 18, strides=[31104, 324, 18, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%481) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:4000:0\n",
      "  %715 : Float(1, 576, 18, 18, strides=[186624, 324, 18, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%482, %716, %717)\n",
      "  %485 : Float(1, 576, 18, 18, strides=[186624, 324, 18, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%715) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %718 : Float(1, 576, 16, 16, strides=[147456, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=576, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%485, %719, %720)\n",
      "  %488 : Float(1, 576, 16, 16, strides=[147456, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%718) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %721 : Float(1, 96, 16, 16, strides=[24576, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%488, %722, %723)\n",
      "  %491 : Float(1, 96, 16, 16, strides=[24576, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Add(%481, %721) # <ipython-input-2-a4271324998c>:77:0\n",
      "  %492 : Float(1, 96, 18, 18, strides=[31104, 324, 18, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%491) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:4000:0\n",
      "  %724 : Float(1, 576, 18, 18, strides=[186624, 324, 18, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%492, %725, %726)\n",
      "  %495 : Float(1, 576, 18, 18, strides=[186624, 324, 18, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%724) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %727 : Float(1, 576, 16, 16, strides=[147456, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=576, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%495, %728, %729)\n",
      "  %498 : Float(1, 576, 16, 16, strides=[147456, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%727) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %730 : Float(1, 160, 16, 16, strides=[40960, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%498, %731, %732)\n",
      "  %501 : Float(1, 160, 20, 20, strides=[64000, 400, 20, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%730) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:4000:0\n",
      "  %733 : Float(1, 960, 20, 20, strides=[384000, 400, 20, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%501, %734, %735)\n",
      "  %504 : Float(1, 960, 20, 20, strides=[384000, 400, 20, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%733) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %736 : Float(1, 960, 16, 16, strides=[245760, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[2, 2], group=960, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%504, %737, %738)\n",
      "  %507 : Float(1, 960, 16, 16, strides=[245760, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%736) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %739 : Float(1, 160, 16, 16, strides=[40960, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%507, %740, %741)\n",
      "  %510 : Float(1, 160, 16, 16, strides=[40960, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Add(%730, %739) # <ipython-input-2-a4271324998c>:77:0\n",
      "  %511 : Float(1, 160, 20, 20, strides=[64000, 400, 20, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%510) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:4000:0\n",
      "  %742 : Float(1, 960, 20, 20, strides=[384000, 400, 20, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%511, %743, %744)\n",
      "  %514 : Float(1, 960, 20, 20, strides=[384000, 400, 20, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%742) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %745 : Float(1, 960, 16, 16, strides=[245760, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[2, 2], group=960, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%514, %746, %747)\n",
      "  %517 : Float(1, 960, 16, 16, strides=[245760, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%745) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %748 : Float(1, 160, 16, 16, strides=[40960, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%517, %749, %750)\n",
      "  %520 : Float(1, 160, 16, 16, strides=[40960, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Add(%510, %748) # <ipython-input-2-a4271324998c>:77:0\n",
      "  %521 : Float(1, 160, 20, 20, strides=[64000, 400, 20, 1], requires_grad=1, device=cuda:0) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%520) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:4000:0\n",
      "  %751 : Float(1, 960, 20, 20, strides=[384000, 400, 20, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%521, %752, %753)\n",
      "  %524 : Float(1, 960, 20, 20, strides=[384000, 400, 20, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%751) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %754 : Float(1, 960, 16, 16, strides=[245760, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[2, 2], group=960, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%524, %755, %756)\n",
      "  %527 : Float(1, 960, 16, 16, strides=[245760, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Clip[max=6., min=0.](%754) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1255:0\n",
      "  %757 : Float(1, 320, 16, 16, strides=[81920, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%527, %758, %759)\n",
      "  %760 : Float(1, 48, 64, 64, strides=[196608, 4096, 64, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%394, %761, %762)\n",
      "  %532 : Float(1, 48, 64, 64, strides=[196608, 4096, 64, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%760) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1204:0\n",
      "  %763 : Float(1, 256, 16, 16, strides=[65536, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%757, %764, %765)\n",
      "  %535 : Float(1, 256, 16, 16, strides=[65536, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%763) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1204:0\n",
      "  %536 : Float(1, 320, 16, 16, strides=[81920, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[6, 6], group=320, kernel_shape=[3, 3], pads=[6, 6, 6, 6], strides=[1, 1]](%757, %classifier.aspp.convs.1.0.body.0.weight) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/modules/conv.py:396:0\n",
      "  %766 : Float(1, 256, 16, 16, strides=[65536, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%536, %767, %768)\n",
      "  %539 : Float(1, 256, 16, 16, strides=[65536, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%766) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1204:0\n",
      "  %540 : Float(1, 320, 16, 16, strides=[81920, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[12, 12], group=320, kernel_shape=[3, 3], pads=[12, 12, 12, 12], strides=[1, 1]](%757, %classifier.aspp.convs.2.0.body.0.weight) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/modules/conv.py:396:0\n",
      "  %769 : Float(1, 256, 16, 16, strides=[65536, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%540, %770, %771)\n",
      "  %543 : Float(1, 256, 16, 16, strides=[65536, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%769) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1204:0\n",
      "  %544 : Float(1, 320, 16, 16, strides=[81920, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[18, 18], group=320, kernel_shape=[3, 3], pads=[18, 18, 18, 18], strides=[1, 1]](%757, %classifier.aspp.convs.3.0.body.0.weight) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/modules/conv.py:396:0\n",
      "  %772 : Float(1, 256, 16, 16, strides=[65536, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%544, %773, %774)\n",
      "  %547 : Float(1, 256, 16, 16, strides=[65536, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%772) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1204:0\n",
      "  %548 : Long(4, strides=[1], device=cpu) = onnx::Shape(%757)\n",
      "  %549 : Long(device=cpu) = onnx::Constant[value={2}]()\n",
      "  %550 : Long(device=cpu) = onnx::Gather[axis=0](%548, %549) # <ipython-input-4-69c644393fd8>:129:0\n",
      "  %551 : Long(4, strides=[1], device=cpu) = onnx::Shape(%757)\n",
      "  %552 : Long(device=cpu) = onnx::Constant[value={3}]()\n",
      "  %553 : Long(device=cpu) = onnx::Gather[axis=0](%551, %552) # <ipython-input-4-69c644393fd8>:129:0\n",
      "  %554 : Float(1, 320, 1, 1, strides=[320, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool(%757) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1037:0\n",
      "  %775 : Float(1, 256, 1, 1, strides=[256, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%554, %776, %777)\n",
      "  %557 : Float(1, 256, 1, 1, strides=[256, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%775) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1204:0\n",
      "  %558 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[axes=[0]](%550)\n",
      "  %559 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[axes=[0]](%553)\n",
      "  %560 : Long(2, strides=[1], device=cpu) = onnx::Concat[axis=0](%558, %559)\n",
      "  %561 : Float(2, strides=[1], device=cpu) = onnx::Constant[value= 1  1 [ CPUFloatType{2} ]]()\n",
      "  %562 : Float(2, strides=[1], device=cpu) = onnx::Cast[to=1](%560)\n",
      "  %563 : Long(4, strides=[1], device=cpu) = onnx::Shape(%557)\n",
      "  %564 : Long(2, strides=[1], device=cpu) = onnx::Slice[axes=[0], ends=[9223372036854775807], starts=[2]](%563)\n",
      "  %565 : Float(2, strides=[1], device=cpu) = onnx::Cast[to=1](%564)\n",
      "  %566 : Float(2, strides=[1], device=cpu) = onnx::Div(%562, %565)\n",
      "  %567 : Float(4, strides=[1], device=cpu) = onnx::Concat[axis=0](%561, %566)\n",
      "  %568 : Float(*, *, *, *, strides=[65536, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Upsample[mode=\"linear\"](%557, %567) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:3554:0\n",
      "  %569 : Float(1, *, 16, 16, strides=[327680, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%535, %539, %543, %547, %568) # <ipython-input-4-69c644393fd8>:161:0\n",
      "  %778 : Float(1, 256, 16, 16, strides=[65536, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%569, %779, %780)\n",
      "  %572 : Float(1, 256, 16, 16, strides=[65536, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%778) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1076:0\n",
      "  %573 : Long(4, strides=[1], device=cpu) = onnx::Shape(%532)\n",
      "  %574 : Long(device=cpu) = onnx::Constant[value={2}]()\n",
      "  %575 : Long(device=cpu) = onnx::Gather[axis=0](%573, %574) # <ipython-input-4-69c644393fd8>:50:0\n",
      "  %576 : Long(4, strides=[1], device=cpu) = onnx::Shape(%532)\n",
      "  %577 : Long(device=cpu) = onnx::Constant[value={3}]()\n",
      "  %578 : Long(device=cpu) = onnx::Gather[axis=0](%576, %577) # <ipython-input-4-69c644393fd8>:50:0\n",
      "  %579 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[axes=[0]](%575)\n",
      "  %580 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[axes=[0]](%578)\n",
      "  %581 : Long(2, strides=[1], device=cpu) = onnx::Concat[axis=0](%579, %580)\n",
      "  %582 : Float(2, strides=[1], device=cpu) = onnx::Constant[value= 1  1 [ CPUFloatType{2} ]]()\n",
      "  %583 : Float(2, strides=[1], device=cpu) = onnx::Cast[to=1](%581)\n",
      "  %584 : Long(4, strides=[1], device=cpu) = onnx::Shape(%572)\n",
      "  %585 : Long(2, strides=[1], device=cpu) = onnx::Slice[axes=[0], ends=[9223372036854775807], starts=[2]](%584)\n",
      "  %586 : Float(2, strides=[1], device=cpu) = onnx::Cast[to=1](%585)\n",
      "  %587 : Float(2, strides=[1], device=cpu) = onnx::Div(%583, %586)\n",
      "  %588 : Float(4, strides=[1], device=cpu) = onnx::Concat[axis=0](%582, %587)\n",
      "  %589 : Float(*, *, *, *, strides=[1048576, 4096, 64, 1], requires_grad=1, device=cuda:0) = onnx::Upsample[mode=\"linear\"](%572, %588) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:3554:0\n",
      "  %590 : Float(1, *, 64, 64, strides=[1245184, 4096, 64, 1], requires_grad=1, device=cuda:0) = onnx::Concat[axis=1](%532, %589) # <ipython-input-4-69c644393fd8>:51:0\n",
      "  %591 : Float(1, 304, 64, 64, strides=[1245184, 4096, 64, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=304, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%590, %classifier.classifier.0.body.0.weight) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/modules/conv.py:396:0\n",
      "  %781 : Float(1, 256, 64, 64, strides=[1048576, 4096, 64, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%591, %782, %783)\n",
      "  %594 : Float(1, 256, 64, 64, strides=[1048576, 4096, 64, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%781) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:1204:0\n",
      "  %595 : Float(1, 19, 64, 64, strides=[77824, 4096, 64, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%594, %classifier.classifier.3.weight, %classifier.classifier.3.bias) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/modules/conv.py:396:0\n",
      "  %596 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[axes=[0]](%363)\n",
      "  %597 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[axes=[0]](%366)\n",
      "  %598 : Long(2, strides=[1], device=cpu) = onnx::Concat[axis=0](%596, %597)\n",
      "  %599 : Float(2, strides=[1], device=cpu) = onnx::Constant[value= 1  1 [ CPUFloatType{2} ]]()\n",
      "  %600 : Float(2, strides=[1], device=cpu) = onnx::Cast[to=1](%598)\n",
      "  %601 : Long(4, strides=[1], device=cpu) = onnx::Shape(%595)\n",
      "  %602 : Long(2, strides=[1], device=cpu) = onnx::Slice[axes=[0], ends=[9223372036854775807], starts=[2]](%601)\n",
      "  %603 : Float(2, strides=[1], device=cpu) = onnx::Cast[to=1](%602)\n",
      "  %604 : Float(2, strides=[1], device=cpu) = onnx::Div(%600, %603)\n",
      "  %605 : Float(4, strides=[1], device=cpu) = onnx::Concat[axis=0](%599, %604)\n",
      "  %606 : Float(*, *, *, *, strides=[1245184, 65536, 256, 1], requires_grad=1, device=cuda:0) = onnx::Upsample[mode=\"linear\"](%595, %605) # /home/aiffel-dj17/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/functional.py:3554:0\n",
      "  return (%606)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model, dummy_input, onnx_model_path, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model = model_map['deeplabv3plus_mobilenet'](num_classes=19, output_stride=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#output = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c5b464f2d684>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchsummary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#summary(model, dummy_input ,batch_size=32, device=device.type)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-038fbcc30085>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                 self._forward_hooks.values()):\n\u001b[0;32m--> 893\u001b[0;31m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36mhook\u001b[0;34m(module, input, output)\u001b[0m\n\u001b[1;32m     24\u001b[0m                 ]\n\u001b[1;32m     25\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0msummary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_shape\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[0msummary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_shape\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "#from torchsummary import summary\n",
    "#summary(model, dummy_input ,batch_size=32, device=device.type)\n",
    "#summary(model,( 3, 256, 256), batch_size=32, device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1+cu102\n"
     ]
    }
   ],
   "source": [
    " print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
